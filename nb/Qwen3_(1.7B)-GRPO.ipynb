{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Time-Carbon/unsloth-test/blob/main/nb/Qwen3_(1.7B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNfFDf6YBPFv"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J19P1l12BPFx"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G-GerIbBPFx"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9LG2faBPFx"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26YWj8B9BPFx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1duDolE2BPFy"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN75nmdx9lvw"
      },
      "source": [
        "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
        "\n",
        "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841,
          "referenced_widgets": [
            "bad11d7a9d244afc9fffffcf01330ec7",
            "598fc44fe4854c09a77371815cc67e51",
            "f83263b44668499197bcc772b99a7c45",
            "9402af12abb34b71a9984bd3a86af750",
            "190b1b82a8854fc8b6e193ec80757bbd",
            "720db6074a024734b561adb085cddc1b",
            "4cf91155369e48fda96a2fad458e785c",
            "627792f37d8d48e38a76ca94486d1fcf",
            "a93b49d8ec874748bd8c4895bed76bf7",
            "8fcbb207100b49df9681128f723f273d",
            "aea88ce443c74d37af4f8997cfd74cc9",
            "c7e9253dfef14c289b908dc48e420cdc",
            "124f366608184718a366032b7add4744",
            "d33cb514fdec4ae6829dcb110c0abf50",
            "500123949e6e48ebbe2c0d248766ee77",
            "0d47afcb4fc042a2a101930e5ef9e800",
            "576da1f0a6dc4c83b2c048973a61e6cd",
            "d4849409d4374ebab6ab7b37b9764e20",
            "4269af5bf35c4175a2317174d6bbdd5b",
            "449f78932a3a47c69427421d3e12d861",
            "f9ba287714994a0ea5b5cc69c78edf05",
            "343032c628a94c8d90e86835ff25a197",
            "97f66167a8bb49c69be54c1da70cef61",
            "079979b5c437498fa34dddc2defc3367",
            "de4f6b11eef14245a1471b7d9e169eac",
            "494f564cbfda4d5fb28fb241681964a8",
            "6226d57df37e4cdc8007d8296daefee9",
            "373ab21479024b85ab55dbf8749aaedc",
            "127ce1a9c2854d7086b8b709d3db0d97",
            "fb9a00750b5f42929acf202ca90f6323",
            "2f1fb93f5b8f4102902d093a8b5177a3",
            "4d6902547261486697ea1c8a016999cd",
            "93b233580f2d44219c0bc6479775d775"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "a00fce6a-6e8c-4231-b8a2-238176fec999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 07-02 10:54:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 07-02 10:54:48 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.52.4. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen3-1.7b-base-bnb-4bit with actual GPU utilization = 49.53%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 5.87 GB. Also swap space = 0 GB.\n",
            "WARNING 07-02 10:55:10 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-02 10:55:30 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 07-02 10:55:30 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
            "INFO 07-02 10:55:30 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-1.7b-base-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-1.7b-base-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-1.7b-base-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
            "INFO 07-02 10:55:32 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 07-02 10:55:32 [cuda.py:289] Using XFormers backend.\n",
            "INFO 07-02 10:55:33 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 07-02 10:55:33 [model_runner.py:1108] Starting to load model unsloth/qwen3-1.7b-base-bnb-4bit...\n",
            "INFO 07-02 10:55:33 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 07-02 10:55:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "INFO 07-02 10:55:35 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bad11d7a9d244afc9fffffcf01330ec7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7e9253dfef14c289b908dc48e420cdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-02 10:55:37 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 07-02 10:55:37 [model_runner.py:1140] Model loading took 1.3120 GiB and 4.065997 seconds\n",
            "INFO 07-02 10:55:41 [worker.py:287] Memory profiling takes 3.13 seconds\n",
            "INFO 07-02 10:55:41 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.50) = 7.30GiB\n",
            "INFO 07-02 10:55:41 [worker.py:287] model weights take 1.31GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 4.91GiB.\n",
            "INFO 07-02 10:55:42 [executor_base.py:112] # cuda blocks: 2873, # CPU blocks: 0\n",
            "INFO 07-02 10:55:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 22.45x\n",
            "INFO 07-02 10:55:42 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/27 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97f66167a8bb49c69be54c1da70cef61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-02 10:56:26 [model_runner.py:1592] Graph capturing finished in 44 secs, took 0.54 GiB\n",
            "INFO 07-02 10:56:26 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 48.16 seconds\n",
            "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
            "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.6.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 16 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-1.7B-Base-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DuiVRLhMco"
      },
      "source": [
        "### GRPO chat template\n",
        "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
        "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
        "2. A `system_prompt` is recommended to at least guide the model's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6UjowCbT-cFz",
        "outputId": "d238313e-ffc5-4683-a602-e5059bed7e11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <answer></answer>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "reasoning_start = \"<think>\" # Acts as <think>\n",
        "reasoning_end   = \"</think>\"   # Acts as </think>\n",
        "solution_start  = \"<answer>\"\n",
        "solution_end    = \"</answer>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgs0MJkDkYL"
      },
      "source": [
        "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y3fF9gMujY02"
      },
      "outputs": [],
      "source": [
        "chat_template = \\\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
        "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages[1:] %}\"\\\n",
        "    \"{% else %}\"\\\n",
        "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages %}\"\\\n",
        "    \"{% endif %}\"\\\n",
        "    \"{% for message in loop_messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ message['content'] }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ message['content'] + eos_token }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "\n",
        "# Replace with out specific template:\n",
        "chat_template = chat_template\\\n",
        "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
        "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcLdymBEHdk"
      },
      "source": [
        "Let's see how our chat template behaves on an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BciEDYSSYFNj",
        "outputId": "6c46019d-e543-4e4e-e153-1d04512baf0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <answer></answer><|endoftext|>What is 1+1?<think>I think it's 2.</think><answer>2</answer><|endoftext|>What is 2+2?<think>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
        "], tokenize = False, add_generation_prompt = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdsuGjxHrjT"
      },
      "source": [
        "### Pre fine-tuning for formatting\n",
        "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
        "\n",
        "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "2e4dbf4b11b943cda8e0f2666a50dc79",
            "df0fad891c4e471d9808039527152cdf",
            "a95866b928dc48d8a23e354b4ce1a408",
            "de9a6ebf23f345beba2624cb1d701201",
            "bac92007aced4a1caa5882dba69544f1",
            "d5925b6505cd401688e5ff68cd73053b",
            "4981ec6235904e35a25523033b19221a",
            "2996111558574bb8b91a7c10772cf403",
            "1533645f5fe848c0bea21d0975339979",
            "ad21692a23ce423aa3230fdaf3d3b386",
            "85361b7f84fa4b799b817e815f201341",
            "be02c75a772d46228f02173078551d07",
            "327d98f474854975800711d2e1325738",
            "92a429a93084473a8dc51515ef06d9de",
            "4156369fbf2440dea7b54f2a35282445",
            "c33ac79a5814407a8361fbac5cd77d1e",
            "3beb0a60e6444bf38a6cb8b7cc05c8be",
            "4ca494dc6f9549bb8a0e4b50e054d91c",
            "d2710b08e2a34211baedbc331166e790",
            "feae6f95c79245cf9d631ec9b879c197",
            "df410c2c825a457ca0e66a6e2005d5e2",
            "dfb1cb75ddfd4b18a5aea52c1a1a625c",
            "1d3b1e12dbec449396dd098133eabfdc",
            "145cf3c6274743d3bb1fb4d5051d27b6",
            "4b1cabf7834a413d91d163777d99c5c2",
            "9114470a123b4846bef5bbb8f9e60115",
            "cf67628ba2be427d96f567202b709e67",
            "230d9141d0f6478d8ee3bcf4d4db8760",
            "307ef9d17ea14654b863a3d89e3b8cfa",
            "b7dd6ec737f34848b8aa82a1fbf1e2ae",
            "d4b3aa619deb4db98cc6709bd245b086",
            "44fe098cbb7c4e03bf516ba9f79a656e",
            "bc45027eadea4fb1a0b2cde4a533e89e"
          ]
        },
        "id": "AXxM2lStVIkd",
        "outputId": "086bd578-b4ae-4168-abf2-14ca0d554b11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/603 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e4dbf4b11b943cda8e0f2666a50dc79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/cot-00000-of-00001.parquet:   0%|          | 0.00/106M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be02c75a772d46228f02173078551d07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating cot split:   0%|          | 0/19252 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d3b1e12dbec449396dd098133eabfdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      expected_answer                                            problem  \\\n",
              "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
              "6                  -2  Find the value of the parameter $a$ for which ...   \n",
              "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
              "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
              "17                 30  What is the largest positive integer that divi...   \n",
              "...               ...                                                ...   \n",
              "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
              "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
              "19247               4  A bus left point X for point Y. Two hours late...   \n",
              "19248              18  Each interior angle of a regular n-gon measure...   \n",
              "19250          0.8960  Find the probability that the second blue resu...   \n",
              "\n",
              "                                      generated_solution  \n",
              "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
              "6      <think>\\nOkay, so I need to find the value of ...  \n",
              "9      <think>\\nOkay, so I need to solve the equation...  \n",
              "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
              "17     <think>\\nAlright, so I need to find the larges...  \n",
              "...                                                  ...  \n",
              "19243  <think>\\nOkay, so I need to find the value of ...  \n",
              "19245  <think>\\nOkay, so I have this problem where a ...  \n",
              "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
              "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
              "19250  <think>\\nOkay, so I need to find the probabili...  \n",
              "\n",
              "[7507 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03e9cd49-d628-4269-8e43-747ecd9f9ac4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>problem</th>\n",
              "      <th>generated_solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-2</td>\n",
              "      <td>Find the value of the parameter $a$ for which ...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>18</td>\n",
              "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>30</td>\n",
              "      <td>What is the largest positive integer that divi...</td>\n",
              "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19243</th>\n",
              "      <td>244</td>\n",
              "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19245</th>\n",
              "      <td>1</td>\n",
              "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19247</th>\n",
              "      <td>4</td>\n",
              "      <td>A bus left point X for point Y. Two hours late...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19248</th>\n",
              "      <td>18</td>\n",
              "      <td>Each interior angle of a regular n-gon measure...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19250</th>\n",
              "      <td>0.8960</td>\n",
              "      <td>Find the probability that the second blue resu...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7507 rows √ó 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03e9cd49-d628-4269-8e43-747ecd9f9ac4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03e9cd49-d628-4269-8e43-747ecd9f9ac4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03e9cd49-d628-4269-8e43-747ecd9f9ac4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-519abb4e-6be6-4d48-b239-5a3aa8ced49c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-519abb4e-6be6-4d48-b239-5a3aa8ced49c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-519abb4e-6be6-4d48-b239-5a3aa8ced49c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 7507,\n  \"fields\": [\n    {\n      \"column\": \"expected_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 868,\n        \"samples\": [\n          \"672\",\n          \"335\",\n          \"575757\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3895,\n        \"samples\": [\n          \"A club with 7 members forms three-person committees, but no two committees can have more than one member in common. What is the maximum number of committees that can be formed?\",\n          \"Find the smallest integer \\\\( a > 2 \\\\) such that \\\\( 2 \\\\mid a \\\\), \\\\( 3 \\\\mid (a+1) \\\\), \\\\( 4 \\\\mid (a+2) \\\\), \\\\( 5 \\\\mid (a+3) \\\\), and \\\\( 6 \\\\mid (a+4) \\\\).\",\n          \"Given the polynomial equation \\\\(x^3 - x = -1\\\\) with roots \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\), find the value of \\\\(\\\\frac{1}{1+a} + \\\\frac{1}{1+b} + \\\\frac{1}{1+c}\\\\).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_solution\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7507,\n        \"samples\": [\n          \"<think>\\nOkay, let's see. I need to solve this problem where there are three prime numbers p, q, and r. The equations given are pq + qr + rp = 191 and p + q = r - 1. The goal is to find p + q + r. Hmm, primes, so they must be 2 or odd primes. Let me start by analyzing the problem step by step.\\n\\nFirst, the equation p + q = r - 1. If I can express r in terms of p and q, maybe I can substitute that into the first equation. Let's try that. So, from the second equation, r = p + q + 1. That seems straightforward. Now, substituting this into the first equation, we get pq + qr + rp = 191. Let's replace r with (p + q + 1).\\n\\nSo, substituting, the first equation becomes:\\n\\npq + q(p + q + 1) + p(p + q + 1) = 191.\\n\\nLet me expand each term:\\n\\nFirst term: pq.\\n\\nSecond term: q*(p + q + 1) = pq + q\\u00b2 + q.\\n\\nThird term: p*(p + q + 1) = p\\u00b2 + pq + p.\\n\\nNow, adding all these together:\\n\\npq + (pq + q\\u00b2 + q) + (p\\u00b2 + pq + p) = 191.\\n\\nCombine like terms:\\n\\npq + pq + pq = 3pq.\\n\\nq\\u00b2 + p\\u00b2.\\n\\nq + p.\\n\\nSo altogether, the equation becomes:\\n\\np\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nHmm. Let me note that.\\n\\nAlternatively, maybe I can factor this expression. Let me see. Let's try to group terms. Wait, another approach: since r = p + q + 1, then p + q + r = (p + q) + (p + q + 1) = 2(p + q) + 1. So if I can find p + q, then multiplying by 2 and adding 1 gives me the answer. That's useful. So perhaps instead of dealing with the first equation directly, I can express it in terms of p + q.\\n\\nBut maybe let's work with the equation we derived: p\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nWait, another thought: p\\u00b2 + q\\u00b2 + 3pq. Let's recall that (p + q)^2 = p\\u00b2 + 2pq + q\\u00b2, so p\\u00b2 + q\\u00b2 + 3pq = (p + q)^2 + pq.\\n\\nTherefore, the equation can be rewritten as:\\n\\n(p + q)^2 + pq + p + q = 191.\\n\\nLet me set S = p + q, and P = pq. Then the equation becomes:\\n\\nS\\u00b2 + P + S = 191.\\n\\nBut from the second equation, we know that r = S + 1, so since we need to find S + r = S + (S + 1) = 2S + 1. So our target is 2S + 1. So if we can find S, we can find the answer.\\n\\nNow, the equation is S\\u00b2 + P + S = 191. But S and P are related as S = p + q, P = pq. For two primes p and q, their sum and product. Since p and q are primes, maybe we can list possible primes that add up to S and multiply to P. However, since S and P are variables here, maybe we can express P in terms of S from the equation.\\n\\nFrom S\\u00b2 + P + S = 191, so P = 191 - S\\u00b2 - S.\\n\\nTherefore, P = -S\\u00b2 - S + 191. But P = pq must be positive, so -S\\u00b2 - S + 191 > 0. Therefore, S\\u00b2 + S < 191. Let's see the possible values of S. Since S is the sum of two primes, which are at least 2 each, so S is at least 2 + 2 = 4. Also, since S\\u00b2 + S < 191, let's solve S\\u00b2 + S - 191 < 0. Let's find the roots of S\\u00b2 + S - 191 = 0.\\n\\nUsing quadratic formula: S = [-1 \\u00b1 sqrt(1 + 4*191)] / 2 = [-1 \\u00b1 sqrt(765)] / 2. sqrt(765) is approx 27.66, so S \\u2248 (-1 + 27.66)/2 \\u2248 13.33. So the positive root is approximately 13.33, so S must be less than 13.33. Therefore, S can be integers from 4 up to 13.\\n\\nSo possible S values: 4,5,6,7,8,9,10,11,12,13.\\n\\nBut since p and q are primes, their sum S must be even or odd. Since except for 2, all primes are odd. So if both p and q are odd primes, their sum is even. If one is 2 and the other is odd, then their sum is odd. So S can be even or odd. So possible values of S (from 4 to 13) can be checked.\\n\\nBut maybe instead of all possible S, let's check possible S values from 4 to 13 and see if for each S, P = 191 - S\\u00b2 - S, and check if P can be expressed as the product of two primes that add up to S.\\n\\nAlternatively, maybe first check which S gives P as a product of two primes.\\n\\nLet's start with S=4. Then P = 191 - 16 -4 = 171. Then check if 171 can be written as product of two primes that add up to 4. But 4 is the sum. The primes could be 2 and 2 (since 2+2=4). Then 2*2=4, but P here is 171. 4 \\u2260 171, so S=4 is invalid.\\n\\nNext S=5: P=191 -25 -5=161. 161 factors into 7*23. Check if 7 + 23 = 30, which is not 5. So that's not possible. Alternatively, are there primes adding to 5? 2 and 3, since 2+3=5. Then P=2*3=6. But 6 \\u2260 161. So S=5 invalid.\\n\\nS=6: P=191 -36 -6=149. 149 is a prime number, so can't be expressed as product of two primes. So invalid.\\n\\nS=7: P=191 -49 -7=135. 135 factors into 5*27 (but 27 not prime), 3*45, 9*15, none primes. So no. So invalid.\\n\\nS=8: P=191 -64 -8=119. 119 factors into 7*17. Check if 7+17=24\\u22608. Primes adding to 8 are 3+5=8 or 5+3. Then P=15. But 15\\u2260119. So invalid.\\n\\nS=9: P=191 -81 -9=101. 101 is prime, so no.\\n\\nS=10: P=191 -100 -10=81. 81=9*9, but 9 not prime. So no.\\n\\nS=11: P=191 -121 -11=59. 59 is prime. So no.\\n\\nS=12: P=191 -144 -12=35. 35=5*7. Check if 5 +7=12? Yes! 5 +7=12. So here, S=12, which is p + q=12, and pq=35. 5 and 7 are primes. So this works.\\n\\nSo then p=5 and q=7, or p=7 and q=5. Then r = S + 1 =12 +1=13. Check if r is prime: 13 is prime. So yes. So then p, q, r are 5,7,13 or 7,5,13. Then p + q + r =5 +7 +13=25. So 25 would be the answer.\\n\\nWait, let me check S=13 as well just to be thorough. S=13: P=191 -169 -13=9. 9=3*3. Check if 3 +3=6\\u226013. So no. So no.\\n\\nTherefore, only S=12 gives valid primes. So the answer is 25.\\n\\nLet me verify the original equations. pq + qr + rp. Let p=5, q=7, r=13. Then 5*7 +7*13 +13*5 =35 +91 +65=35+91=126+65=191. Which matches. And p + q =5 +7=12. r -1=13 -1=12. So that also matches. So it's correct. Therefore, the answer is 25.\\n</think>To solve the problem where three prime numbers \\\\( p, q, \\\\) and \\\\( r \\\\) satisfy the equations \\\\( pq + qr + rp = 191 \\\\) and \\\\( p + q = r - 1 \\\\), we proceed as follows:\\n\\n1. **Express \\\\( r \\\\) in terms of \\\\( p \\\\) and \\\\( q \\\\):**\\n   From the equation \\\\( p + q = r - 1 \\\\), we can solve for \\\\( r \\\\):\\n   \\\\[\\n   r = p + q + 1\\n   \\\\]\\n\\n2. **Substitute \\\\( r \\\\) into the first equation:**\\n   Substitute \\\\( r = p + q + 1 \\\\) into the equation \\\\( pq + qr + rp = 191 \\\\):\\n   \\\\[\\n   pq + q(p + q + 1) + p(p + q + 1) = 191\\n   \\\\]\\n   Expanding and combining like terms:\\n   \\\\[\\n   pq + pq + q^2 + q + p^2 + pq + p = 191\\n   \\\\]\\n   Simplify:\\n   \\\\[\\n   p^2 + q^2 + 3pq + p + q = 191\\n   \\\\]\\n\\n3. **Introduce new variables:**\\n   Let \\\\( S = p + q \\\\) and \\\\( P = pq \\\\). The equation becomes:\\n   \\\\[\\n   S^2 + P + S = 191\\n   \\\\]\\n\\n4. **Express \\\\( r \\\\) in terms of \\\\( S \\\\):**\\n   Since \\\\( r = p + q + 1 = S + 1 \\\\), we need to find \\\\( S \\\\) such that \\\\( S^2 + P + S = 191 \\\\) and \\\\( P = pq \\\\) is the product of two primes \\\\( p \\\\) and \\\\( q \\\\) that sum to \\\\( S \\\\).\\n\\n5. **Determine possible values for \\\\( S \\\\):**\\n   Solve the inequality \\\\( S^2 + S < 191 \\\\):\\n   \\\\[\\n   S^2 + S - 191 < 0\\n   \\\\]\\n   Using the quadratic formula \\\\( S = \\\\frac{-1 \\\\pm \\\\sqrt{1 + 4 \\\\cdot 191}}{2} \\\\):\\n   \\\\[\\n   S = \\\\frac{-1 \\\\pm \\\\sqrt{765}}{2}\\n   \\\\]\\n   Since \\\\( \\\\sqrt{765} \\\\approx 27.66 \\\\), we have:\\n   \\\\[\\n   S \\\\approx \\\\frac{-1 + 27.66}{2} \\\\approx 13.33\\n   \\\\]\\n   Therefore, \\\\( S \\\\) must be an integer between 4 and 13.\\n\\n6. **Check possible values of \\\\( S \\\\):**\\n   - For \\\\( S = 12 \\\\):\\n     \\\\[\\n     P = 191 - 12^2 - 12 = 191 - 144 - 12 = 35\\n     \\\\]\\n     Check if \\\\( 35 \\\\) can be written as the product of two primes that sum to 12:\\n     \\\\[\\n     35 = 5 \\\\times 7 \\\\quad \\\\text{and} \\\\quad 5 + 7 = 12\\n     \\\\]\\n     This works. So \\\\( p = 5 \\\\) and \\\\( q = 7 \\\\).\\n\\n7. **Find \\\\( r \\\\):**\\n   \\\\[\\n   r = S + 1 = 12 + 1 = 13\\n   \\\\]\\n\\n8. **Verify the solution:**\\n   - Check \\\\( pq + qr + rp = 191 \\\\):\\n     \\\\[\\n     5 \\\\times 7 + 7 \\\\times 13 + 13 \\\\times 5 = 35 + 91 + 65 = 191\\n     \\\\]\\n   - Check \\\\( p + q = r - 1 \\\\):\\n     \\\\[\\n     5 + 7 = 12 \\\\quad \\\\text{and} \\\\quad 13 - 1 = 12\\n     \\\\]\\n\\nSince all conditions are satisfied, the final answer is:\\n\\\\[\\n\\\\boxed{25}\\n\\\\]\",\n          \"<think>\\nOkay, let's see. I need to solve this problem where x and y are positive integers satisfying 2(x + y) = gcd(x, y) + lcm(x, y). And I have to find the ratio of the lcm to the gcd of x and y. Hmm, okay, let's break this down.\\n\\nFirst, I remember that for any two positive integers, the product of the lcm and gcd of those numbers is equal to the product of the numbers themselves. So, lcm(x, y) * gcd(x, y) = x * y. That might come in handy here. Let me note that down: lcm(x,y)*gcd(x,y) = x*y.\\n\\nGiven the equation 2(x + y) = gcd(x, y) + lcm(x, y), maybe I can express everything in terms of gcd and the ratio of x and y. Since gcd and lcm are involved, it might be helpful to let d = gcd(x, y), and then express x and y as x = d*a and y = d*b, where a and b are coprime integers (their gcd is 1). That's a standard approach for problems involving gcd and lcm.\\n\\nSo let me set d = gcd(x, y). Then x = d*a, y = d*b, with gcd(a, b) = 1. Then, the lcm(x, y) would be d*a*b, because lcm(x, y) = x*y / gcd(x, y) = (d*a*d*b)/d = d*a*b. Right, so lcm(x,y) = d*a*b.\\n\\nSubstituting these into the original equation: 2(x + y) = gcd(x, y) + lcm(x, y).\\n\\nSubstituting x = d*a, y = d*b, gcd = d, lcm = d*a*b. Then:\\n\\n2(d*a + d*b) = d + d*a*b.\\n\\nFactor out d from the left side: 2d(a + b) = d(1 + a*b).\\n\\nSince d is a positive integer, we can divide both sides by d, yielding:\\n\\n2(a + b) = 1 + a*b.\\n\\nSo now the equation simplifies to 2(a + b) = a*b + 1, where a and b are coprime positive integers. Hmm, okay. Now we have a simpler equation to solve: a*b - 2a - 2b + 1 = 0. Let me rearrange that:\\n\\na*b - 2a - 2b + 1 = 0.\\n\\nHmm, maybe factor this equation? Let me see. Adding 4 to both sides might help in factoring. Let's try:\\n\\na*b - 2a - 2b + 1 + 4 - 4 = 0\\n\\nSo, a*b - 2a - 2b + 4 = 3.\\n\\nWait, not sure. Alternatively, perhaps rearrange the terms:\\n\\na*b - 2a - 2b = -1.\\n\\nThen, add 4 to both sides:\\n\\na*b - 2a - 2b + 4 = 3.\\n\\nNow, left side can be factored as (a - 2)(b - 2) = 3. Because expanding (a - 2)(b - 2) gives a*b - 2a - 2b + 4. Yes, that's right. So:\\n\\n(a - 2)(b - 2) = 3.\\n\\nSince a and b are positive integers and coprime, we need to find pairs (a, b) such that their product is 3 when each is reduced by 2. Also, since a and b are coprime, (a - 2) and (b - 2) must be divisors of 3, which is prime. The positive divisors of 3 are 1 and 3.\\n\\nSo possible pairs (since a and b are positive integers, a - 2 and b - 2 must be at least such that a and b are positive. Let's see:\\n\\nCase 1: (a - 2) = 1 and (b - 2) = 3. Then, a = 3, b = 5. Check if gcd(a, b) = 1. gcd(3, 5) = 1, which is good.\\n\\nCase 2: (a - 2) = 3 and (b - 2) = 1. Then, a = 5, b = 3. Similarly, gcd(5, 3) = 1. So this is also valid.\\n\\nBut also, since 3 is prime, the only positive divisors are 1 and 3. But since we're considering positive integers, we could also consider if one of them is negative? But since a and b are positive, a - 2 and b - 2 must be positive or zero? Wait, but 3 is positive, so the factors must both be positive. Because if one of (a - 2) or (b - 2) were negative, their product would be negative, but 3 is positive. So both (a - 2) and (b - 2) must be positive. Thus, only the two cases above.\\n\\nAlternatively, maybe (a - 2) and (b - 2) could be 3 and 1 in some order, which gives the two cases. So the possible (a, b) are (3, 5) and (5, 3). Since a and b are interchangeable (since x and y are symmetric in the problem), these two cases would yield the same results.\\n\\nSo now, let's see. For (a, b) = (3, 5), then x = d*3, y = d*5. Similarly, for (a, b) = (5, 3), x = d*5, y = d*3. But since the problem is symmetric in x and y, both cases are equivalent.\\n\\nNow, since we need to find the ratio lcm(x, y)/gcd(x, y), let's compute that.\\n\\nRecall that lcm(x, y)/gcd(x, y) = (d*a*b)/d = a*b. So it's simply a*b. Since in both cases, a and b are 3 and 5, the product is 15. Therefore, the ratio is 15.\\n\\nWait, that seems too straightforward. Let me check.\\n\\nIf the ratio is a*b, then yes. Because lcm(x, y) is d*a*b and gcd(x, y) is d, so their ratio is (d*a*b)/d = a*b. Since a and b are 3 and 5, 3*5=15. Therefore, the answer is 15. So the answer is 15.\\n\\nBut let me verify with an example. Let's take d=1. Then x=3, y=5. Then gcd(3,5)=1, lcm=15. Then 2(x + y) = 2*(8) = 16. The right side is 1 + 15=16. So that works. If d=1, then 2(3+5)=16=1 +15.\\n\\nWhat if d=2? Then x=6, y=10. gcd(6,10)=2, lcm=30. Then 2(6 + 10)=2*16=32. The right side is 2 +30=32. So that also works. Then the ratio lcm/gcd is 30/2=15. So regardless of d, the ratio is always 15. Wait, because if x = d*a, y = d*b, then lcm(x,y)/gcd(x,y) = (d*a*b)/d = a*b, which is 15 as in the first case. So regardless of d, the ratio is always 15. Therefore, the answer is 15.\\n\\nSo even if d is some other positive integer, the ratio remains a*b =15. Therefore, the required ratio is 15.\\n\\nTherefore, the answer is 15. So \\\\boxed{15}.\\n\\n**Final Answer**\\n\\\\boxed{15}\\n</think>Given \\\\( x \\\\) and \\\\( y \\\\) are positive integers such that \\\\( 2(x + y) = \\\\gcd(x, y) + \\\\text{lcm}(x, y) \\\\), we need to find \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\).\\n\\nFirst, let \\\\( d = \\\\gcd(x, y) \\\\). Then, we can express \\\\( x \\\\) and \\\\( y \\\\) as \\\\( x = d \\\\cdot a \\\\) and \\\\( y = d \\\\cdot b \\\\), where \\\\( \\\\gcd(a, b) = 1 \\\\). The least common multiple (lcm) of \\\\( x \\\\) and \\\\( y \\\\) is given by \\\\( \\\\text{lcm}(x, y) = d \\\\cdot a \\\\cdot b \\\\).\\n\\nSubstituting these into the given equation:\\n\\\\[\\n2(d \\\\cdot a + d \\\\cdot b) = d + d \\\\cdot a \\\\cdot b\\n\\\\]\\nDividing both sides by \\\\( d \\\\):\\n\\\\[\\n2(a + b) = 1 + a \\\\cdot b\\n\\\\]\\nRearranging terms, we get:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 1 = 0\\n\\\\]\\nAdding 4 to both sides to factorize:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 4 = 3\\n\\\\]\\nThis can be factored as:\\n\\\\[\\n(a - 2)(b - 2) = 3\\n\\\\]\\nThe positive integer solutions for \\\\((a - 2)\\\\) and \\\\((b - 2)\\\\) are 1 and 3. Thus, the possible pairs \\\\((a, b)\\\\) are \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\). Since \\\\( a \\\\) and \\\\( b \\\\) are coprime, both pairs are valid.\\n\\nThe ratio \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\) is given by:\\n\\\\[\\n\\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} = \\\\frac{d \\\\cdot a \\\\cdot b}{d} = a \\\\cdot b\\n\\\\]\\nFor both pairs \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\), the product \\\\( a \\\\cdot b = 15 \\\\).\\n\\nThus, the final answer is:\\n\\\\[\\n\\\\boxed{15}\\n\\\\]\",\n          \"<think>\\nOkay, so I need to find the remainder when the product of all odd numbers from 1 to 2005 is divided by 1000. Hmm, let's think about how to approach this. \\n\\nFirst, the product is 1 \\u00d7 3 \\u00d7 5 \\u00d7 ... \\u00d7 2005. That's a lot of numbers! Since we're dealing with division by 1000, maybe modular arithmetic can help here. The remainder when divided by 1000 is equivalent to the product modulo 1000. But calculating such a huge product directly seems impossible. There has to be a smarter way.\\n\\nI remember that when dealing with factorials and remainders, factors of 2 and 5 can create trailing zeros. However, here we're only multiplying odd numbers, so there are no factors of 2. But there might still be factors of 5. Wait, 1000 is 8\\u00d7125, which is 2^3 \\u00d7 5^3. Since the product is all odd numbers, it won't have factors of 2, but it can have factors of 5. Therefore, the product will be divisible by 5^3, but since there are no 2s, the product modulo 1000 might not be zero. Hmm, maybe I need to compute the product modulo 1000, but adjusting for the factors of 5?\\n\\nAlternatively, maybe split the problem into modulo 8 and modulo 125, then use the Chinese Remainder Theorem (CRT) to combine the results. Since 1000 = 8 \\u00d7 125, and 8 and 125 are coprime, CRT says that if I can find the remainder modulo 8 and modulo 125, then I can combine them to find the remainder modulo 1000. That might be a good approach.\\n\\nLet me start with modulo 8. The product is 1\\u00d73\\u00d75\\u00d77\\u00d79\\u00d7...\\u00d72005. But modulo 8, odd numbers repeat every 8 numbers. Let's see, the residues modulo 8 of odd numbers are 1,3,5,7,1,3,5,7,... So the pattern repeats every 4 terms. Wait, no. Wait, the numbers go 1,3,5,7,9\\u22611,11\\u22613,13\\u22615,15\\u22617, etc. So every 8 numbers, the cycle of residues 1,3,5,7 repeats twice. Wait, actually, modulo 8, the odd residues cycle every 4 numbers. Let's confirm:\\n\\n1 mod 8 =1\\n\\n3 mod8=3\\n\\n5 mod8=5\\n\\n7 mod8=7\\n\\n9 mod8=1\\n\\n11 mod8=3\\n\\n13 mod8=5\\n\\n15 mod8=7\\n\\nYes, every 4 terms, the cycle repeats. So how many terms are in the product 1\\u00d73\\u00d75\\u00d7...\\u00d72005? Let's find the number of terms first. The nth odd number is 2n-1. So 2n-1=2005 => n=(2005+1)/2=2006/2=1003. So there are 1003 terms.\\n\\nSo 1003 terms, each group of 4 terms (mod8) is 1\\u00d73\\u00d75\\u00d77=105. Then 105 mod8= 105 - 13\\u00d78=105-104=1. So each group of 4 terms multiplies to 1 mod8. Then how many full groups of 4 are there in 1003 terms? Let's divide 1003 by 4. 1003 \\u00f74=250.75. So 250 full groups, each contributing 1 mod8, and then a remainder of 3 terms. \\n\\nSo the total product modulo8 is (1^250) \\u00d7 (last three terms). The last three terms would be the terms after the 250th group. The 250th group ends at term 250\\u00d74=1000. So the 1001st term is 2\\u00d71001 -1=2001. Wait, no: the first term is 1=2\\u00d71-1, second term 3=2\\u00d72-1, so term k is 2k-1. Therefore, term 1001 is 2\\u00d71001 -1=2002-1=2001. Then the 1002nd term is 2003, 1003rd term is 2005. So the last three terms are 2001, 2003, 2005. Let's compute each mod8:\\n\\n2001 \\u00f78: 8\\u00d7250=2000, so 2001 mod8=1\\n\\n2003 mod8=3\\n\\n2005 mod8=5\\n\\nSo the last three terms modulo8 are 1\\u00d73\\u00d75=15 mod8=7.\\n\\nTherefore, total product mod8 is (1^250) \\u00d77=1\\u00d77=7 mod8.\\n\\nSo the remainder modulo8 is7.\\n\\nNow, we need to compute the product modulo125. This seems more complicated. Let's think.\\n\\nThe product is the product of all odd numbers from1 to2005. Wait, 2005=5\\u00d7401. So we can write the product as (1\\u00d73\\u00d75\\u00d77\\u00d7...\\u00d72005). Let's note that there are a lot of factors of 5 in this product, which would make the product divisible by 5 multiple times. However, modulo125 is 5^3, so if the product has at least three factors of 5, then modulo125 would be 0. Wait, but maybe even if it's divisible by 5^3, but we need to compute the actual remainder. Wait, but perhaps the product is divisible by 5^3, but when divided by 5^3, the remaining product modulo8 or something else. Wait, maybe not. Let me check how many factors of5 are in the product.\\n\\nThe number of factors of5 in the product:\\n\\nEach multiple of5 contributes at least one factor of5. Since we're dealing with odd numbers, the multiples of5 that are odd. So numbers divisible by5 but not by2. So numbers like5,15,25,...,2005. Let's count how many multiples of5 are in the product. The first term is5, which is5\\u00d71, then15=5\\u00d73,..., up to2005=5\\u00d7401. So the multiples of5 are5\\u00d7(1,3,5,...,401). Wait, 5\\u00d7k, where k is odd from1 to401. Because 5\\u00d7401=2005. So how many terms are there?\\n\\nThe number of terms k from1 to401 where k is odd. Since401 is odd, the number is (401 +1)/2=201. So there are201 multiples of5 in the product. Each contributes at least one factor of5. Additionally, multiples of25 contribute an extra factor of5. Similarly, multiples of125, 625, etc., contribute more factors.\\n\\nSo let's compute the total number of factors of5 in the product.\\n\\nNumber of multiples of5:201 (as above)\\n\\nNumber of multiples of25: These are numbers in the product divisible by25. Since the product includes numbers of the form5\\u00d7(odd numbers). So multiples of25 are numbers divisible by25, which are 25,75,125,...,2000. But 2005 is not divisible by25. Wait, wait, in the original product (all odd numbers up to2005), the multiples of25 must be odd multiples. So 25\\u00d71,25\\u00d73,..., up to the largest odd multiple less than or equal to2005.\\n\\n25\\u00d7k \\u22642005, where k is odd. Let's compute k_max:\\n\\n25k \\u22642005 => k \\u22642005/25=80.2. So the largest integer k is80, but since k has to be odd, the largest odd k is79. So 25\\u00d779=1975. Then 25\\u00d781=2025, which is over. So the multiples of25 in the product are25\\u00d71,25\\u00d73,...,25\\u00d779. Number of terms: (79-1)/2 +1=39 +1=40. Wait, from1 to79 odd numbers: number is (79+1)/2=40. So there are40 multiples of25.\\n\\nSimilarly, multiples of125:125\\u00d71,125\\u00d73,..., up to125\\u00d7k\\u22642005. 125\\u00d7k \\u22642005 =>k\\u226416.04. So k_max=15 (odd). So 125\\u00d715=1875. So multiples are125\\u00d71,125\\u00d73,...,125\\u00d715. Number of terms: (15-1)/2 +1=7+1=8.\\n\\nMultiples of625:625\\u00d71=625, next is625\\u00d73=1875, next is625\\u00d75=3125>2005. So only two multiples:625 and1875. But1875 is already counted as a multiple of125. So factors of625 contribute an extra factor of5 each. So number of multiples of625 is 2. 625 and1875.\\n\\nMultiples of3125:3125>2005, so none.\\n\\nSo total number of factors of5:\\n\\nFrom multiples of5:201\\n\\nFrom multiples of25:40 (each contributes an extra)\\n\\nFrom multiples of125:8 (each contributes another extra)\\n\\nFrom multiples of625:2 (each contributes another extra)\\n\\nTotal:201 +40 +8 +2=251.\\n\\nSo total factors of5 in the product:251.\\n\\nSimilarly, factors of2: since all numbers are odd, there are none. So the product is divisible by5^251 but not by2. So when dividing by5^3, since 251\\u22653, the product is divisible by5^3. Therefore, the product modulo125 is0? Wait, no, wait. Wait, 125 is5^3. If the product has at least three factors of5, then when divided by5^3, the quotient is an integer, but the remainder when divided by125 is0. So if the product is divisible by125, then the remainder is0. But the question is, when we divide the product by1000, which is8\\u00d7125, the remainder is to be found. However, we already considered that modulo8 is7, and modulo125 is... Hmm, but if modulo125 is0, then using CRT, we can say the remainder is a number congruent to7 mod8 and0 mod125. So we need to solve for x\\u22610 mod125 andx\\u22617 mod8.\\n\\nBut wait, let me confirm if modulo125 is indeed0. Let's check: since the product has at least three factors of5, then yes, the product is divisible by5^3, so product \\u22610 mod125. Therefore, modulo125 is0. Therefore, we have:\\n\\nx \\u22617 mod8\\n\\nx \\u22610 mod125\\n\\nWe need to find x such that x \\u22610 mod125 andx \\u22617 mod8. Let\\u2019s solve this system.\\n\\nLet x=125k. Then 125k\\u22617 mod8. Since125 mod8=5, so 5k\\u22617 mod8.\\n\\nWe solve 5k\\u22617 mod8.\\n\\nMultiply both sides by inverse of5 mod8. The inverse of5 mod8 is5, since5\\u00d75=25\\u22611 mod8. So multiply both sides by5:\\n\\nk\\u22617\\u00d75 mod8 =>k\\u226135 mod8 =>35\\u00f78=4*8=32, 35-32=3. So k\\u22613 mod8.\\n\\nTherefore, k=8m +3 for some integer m. Therefore, x=125(8m +3)=1000m +375. Therefore, the smallest non-negative solution is375. Therefore, the remainder is375 when divided by1000. So the answer is375.\\n\\nWait, but before accepting that, let me verify my steps again because this is tricky.\\n\\nFirst, confirming that the product is divisible by5^3: yes, since there are 251 factors of5, so 5^251 divides the product. Therefore, the product is divisible by5^3, so product\\u22610 mod125. That's correct.\\n\\nThen, solving x\\u22617 mod8 and x\\u22610 mod125. So x=125k. Then 125k\\u22615k mod8. So 5k\\u22617 mod8. Multiply both sides by inverse of5 mod8, which is5, since5\\u00d75=25\\u22611 mod8. So k\\u226135 mod8\\u22613 mod8. Therefore, k=8m +3, so x=125\\u00d73 +1000m=375 +1000m. So the minimal positive solution is375. So remainder is375 when divided by1000. That seems correct.\\n\\nBut wait, to make sure, let me check with an example. Let's take x=375. 375 \\u00f78=46*8=368, remainder7. So 375 mod8=7. 375 mod125=0. So yes, 375 satisfies both conditions. Therefore, the remainder is375. Therefore, the answer is\\\\boxed{375}.\\n\\nWait, but I need to make sure that my calculation for modulo8 and modulo125 are correct. Let me double-check the modulo8 calculation.\\n\\nEarlier, I considered that the product modulo8 is7. Let me recast that.\\n\\nThe product is1\\u00d73\\u00d75\\u00d77\\u00d7\\u2026\\u00d72005. When taking modulo8, each cycle of4 terms (1,3,5,7) multiplies to1\\u00d73\\u00d75\\u00d77=105\\u22611 mod8. Since 105/8=13\\u00d78=104, 105-104=1. Then, since there are1003 terms, how many cycles of4 are there? 1003 divided by4 is250 cycles with a remainder of3 terms. Then the product is (1)^250 \\u00d7 last3 terms.\\n\\nThe last3 terms are2001,2003,2005. 2001 mod8=1 (2001-8\\u00d7250=2001-2000=1), 2003=2001+2\\u21921+2=3 mod8, 2005=2001+4\\u21921+4=5 mod8. So last3 terms are1\\u00d73\\u00d75=15\\u22617 mod8. Therefore, total product\\u22611^250 \\u00d77\\u22617 mod8. Correct.\\n\\nTherefore, the logic holds. Therefore, the remainder is375. So I think that's the correct answer.\\n\\n**Final Answer**\\n\\\\boxed{375}\\n</think>To find the remainder when the product \\\\(1 \\\\times 3 \\\\times 5 \\\\times \\\\cdots \\\\times 2005\\\\) is divided by 1000, we use modular arithmetic and the Chinese Remainder Theorem (CRT).\\n\\n### Step 1: Calculate the product modulo 8\\n\\nThe sequence of odd numbers modulo 8 repeats every 4 terms: \\\\(1, 3, 5, 7\\\\). The product of each cycle is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 \\\\times 7 = 105 \\\\equiv 1 \\\\mod 8\\n\\\\]\\n\\nThere are 1003 terms in the product. Dividing 1003 by 4 gives 250 full cycles and a remainder of 3 terms. The remaining terms are 2001, 2003, and 2005. We calculate these modulo 8:\\n\\\\[\\n2001 \\\\equiv 1 \\\\mod 8, \\\\quad 2003 \\\\equiv 3 \\\\mod 8, \\\\quad 2005 \\\\equiv 5 \\\\mod 8\\n\\\\]\\nThe product of these remaining terms is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 = 15 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThus, the product modulo 8 is:\\n\\\\[\\n1^{250} \\\\times 7 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\n### Step 2: Calculate the product modulo 125\\n\\nWe need to determine the number of factors of 5 in the product. The sequence of odd numbers includes multiples of 5, 25, 125, and 625.\\n\\n- Multiples of 5: \\\\(5, 15, 25, \\\\ldots, 2005\\\\)\\n  - These are of the form \\\\(5 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 200\\\\)\\n  - Number of such terms: \\\\(\\\\frac{2005}{5} = 401\\\\), and half of these are odd, so \\\\(201\\\\) multiples of 5.\\n\\n- Multiples of 25: \\\\(25, 75, 125, \\\\ldots, 1975\\\\)\\n  - These are of the form \\\\(25 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 39\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1975}{25} = 79\\\\), and half of these are odd, so \\\\(40\\\\) multiples of 25.\\n\\n- Multiples of 125: \\\\(125, 375, 625, 875, 1125, 1375, 1625, 1875\\\\)\\n  - These are of the form \\\\(125 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 15\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{125} = 15\\\\), and half of these are odd, so \\\\(8\\\\) multiples of 125.\\n\\n- Multiples of 625: \\\\(625, 1875\\\\)\\n  - These are of the form \\\\(625 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{625} = 3\\\\), and half of these are odd, so \\\\(2\\\\) multiples of 625.\\n\\nTotal factors of 5:\\n\\\\[\\n201 + 40 + 8 + 2 = 251\\n\\\\]\\n\\nSince \\\\(251 \\\\geq 3\\\\), the product is divisible by \\\\(5^3 = 125\\\\). Therefore, the product modulo 125 is:\\n\\\\[\\n0 \\\\mod 125\\n\\\\]\\n\\n### Step 3: Combine results using the Chinese Remainder Theorem\\n\\nWe have:\\n\\\\[\\nx \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\\\[\\nx \\\\equiv 0 \\\\mod 125\\n\\\\]\\n\\nLet \\\\(x = 125k\\\\). Then:\\n\\\\[\\n125k \\\\equiv 7 \\\\mod 8\\n\\\\]\\nSince \\\\(125 \\\\equiv 5 \\\\mod 8\\\\), we have:\\n\\\\[\\n5k \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThe multiplicative inverse of 5 modulo 8 is 5, so:\\n\\\\[\\nk \\\\equiv 7 \\\\times 5 \\\\equiv 35 \\\\equiv 3 \\\\mod 8\\n\\\\]\\n\\nThus, \\\\(k = 8m + 3\\\\) for some integer \\\\(m\\\\). Therefore:\\n\\\\[\\nx = 125(8m + 3) = 1000m + 375\\n\\\\]\\n\\nThe smallest non-negative solution is:\\n\\\\[\\nx = 375\\n\\\\]\\n\\nTherefore, the remainder when the product is divided by 1000 is:\\n\\\\[\\n\\\\boxed{375}\\n\\\\]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
        "dataset = dataset.to_pandas()[\n",
        "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
        "]\n",
        "\n",
        "# Try converting to number - if not, replace with NaN\n",
        "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
        "# Select only numbers\n",
        "dataset = dataset.iloc[np.where(is_number)[0]]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRFqoSdIEVK"
      },
      "source": [
        "We have to format the dataset to follow our GRPO style formatting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z9ydcV_Abfi6"
      },
      "outputs": [],
      "source": [
        "def format_dataset(x):\n",
        "    expected_answer = x[\"expected_answer\"]\n",
        "    problem = x[\"problem\"]\n",
        "\n",
        "    # Remove generated <think> and </think>\n",
        "    thoughts = x[\"generated_solution\"]\n",
        "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
        "\n",
        "    # Strip newlines on left and right\n",
        "    thoughts = thoughts.strip()\n",
        "    # Add our custom formatting\n",
        "    final_prompt = \\\n",
        "        reasoning_start + thoughts + reasoning_end + \\\n",
        "        solution_start + expected_answer + solution_end\n",
        "    return [\n",
        "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
        "        {\"role\" : \"user\",      \"content\" : problem},\n",
        "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
        "    ]\n",
        "\n",
        "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5NI47rOIRP2"
      },
      "source": [
        "Check to see if it worked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "LTdXBKcslhRH",
        "outputId": "07a70f8f-d815-4997-b926-1c7f8429785e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <answer></answer><|endoftext|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<think>Okay, let's see. I need to solve the equation ‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n‚àö(x¬≤ + 165) = 7 + ‚àö(x¬≤ - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(‚àö(x¬≤ + 165))¬≤ = (7 + ‚àö(x¬≤ - 52))¬≤.\\n\\nSimplifying the left side:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤.\\n\\nThe right side is expanded using the formula (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So the right side becomes 7¬≤ + 2*7*‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤, which is 49 + 14‚àö(x¬≤ - 52) + (x¬≤ - 52).\\n\\nSo putting it all together:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + x¬≤ - 52.\\n\\nHmm, let's simplify the right side. The x¬≤ terms will cancel out, right? Let's subtract x¬≤ from both sides:\\n\\n165 = 49 + 14‚àö(x¬≤ - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14‚àö(x¬≤ - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14‚àö(x¬≤ - 52).\\n\\nSo 168 = 14‚àö(x¬≤ - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = ‚àö(x¬≤ - 52).\\n\\n12 = ‚àö(x¬≤ - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12¬≤ = x¬≤ - 52.\\n\\n144 = x¬≤ - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x¬≤.\\n\\n196 = x¬≤.\\n\\nSo x = ‚àö196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n‚àö(14¬≤ + 165) - ‚àö(14¬≤ - 52) = ?\\n\\nCalculate each term:\\n\\n14¬≤ is 196.\\n\\nSo first radical: ‚àö(196 + 165) = ‚àö361 = 19.\\n\\nSecond radical: ‚àö(196 - 52) = ‚àö144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]</think><answer>14</answer><|endoftext|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHV9BXYiIYaq"
      },
      "source": [
        "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
        "\n",
        "Note this might take 2 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBHFlRbae9_s",
        "outputId": "451f668f-e6ba-453d-e665-1d0f1b277066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
        "\n",
        "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6NkUCAGIj8N"
      },
      "source": [
        "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgdtiV_f5hx",
        "outputId": "64e1e56d-327f-449b-a684-400f7890f29a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['expected_answer', 'problem', 'generated_solution', 'Messages', 'N', 'text', '__index_level_0__'],\n",
              "    num_rows: 64\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
        "dataset = Dataset.from_pandas(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQJjQrYKzOk"
      },
      "source": [
        "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a501bcd9abe7477ebe38c981d89ac091",
            "37d9daf823de43caa7be1916f862d7d9",
            "f1c313adf0164cc49b3051489105e90e",
            "b164b9fd9a5f49cc86814efcbe4386be",
            "a4a40a8ef0fe45c29a287a8da5144c3d",
            "e39504676e824c7da81f9e653f25824e",
            "61b6d4244ab14c668a530e75919e79a0",
            "b9cd78ad931f43cca514b31da6ee2813",
            "5a649c1cc87d4f2ab4f6794850500343",
            "d9435ca5b5574041a45819446e502f69",
            "5f951cac3fb64a22939096e2ed01cd53"
          ]
        },
        "id": "woYi0SSygpqp",
        "outputId": "a24ea72a-d5a4-4d59-ea27-a9f5b95c4aaf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/64 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a501bcd9abe7477ebe38c981d89ac091"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        learning_rate = 1e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l4-2v_bLhZuE",
        "outputId": "e45fba6e-5e7e-4f08-abce-4d567df102d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 64 | Num Epochs = 2 | Total steps = 128\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
            " \"-____-\"     Trainable parameters = 17,432,576 of 1,700,000,000 (1.03% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.967700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.846700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.742400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.586200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.670500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.608900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.574500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.613700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.470900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.476900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.462900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.459900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.528200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.494900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.521200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=128, training_loss=0.5734056662768126, metrics={'train_runtime': 92.3541, 'train_samples_per_second': 1.386, 'train_steps_per_second': 1.386, 'total_flos': 500163632934912.0, 'train_loss': 0.5734056662768126})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRMBNUBgLC8T"
      },
      "source": [
        "Let's check if the model has learnt to follow the custom format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJxrS76h3Ds",
        "outputId": "359836d6-83ed-4cfa-c9f7-0c368c4162f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <think> and </think>.\n",
            "Then, provide your solution between <answer></answer><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<think>Okay, let's see. Jenifer has 82 cents in pennies and nickels. Her brother thought all the nickels were dimes and counted the total as $1.47. I need to find out how many pennies Jenifer has.\n",
            "\n",
            "First, let me convert the total amount to cents. Since $1 is 100 cents, $1.47 is 147 cents. So, the brother's count of 147 cents is the total amount, but he mistook the nickels for dimes. That means the number of nickels he counted is actually the number of dimes he thought they were.\n",
            "\n",
            "Let me denote the number of nickels as N and the number of pennies as P. The total amount is 82 cents, so the equation for that is:\n",
            "\n",
            "N + P = 82\n",
            "\n",
            "But the brother counted the nickels as dimes, so he thought there were 147 cents. Since each nickel is 5 cents and each dime is 10 cents, the equation for the brother's count is:\n",
            "\n",
            "5N (nickels as dimes) + 10N (dimes as dimes) = 147\n",
            "\n",
            "Wait, but wait. The brother thought all nickels were dimes. So if he counted N nickels as dimes, then each nickel would be counted as 10 cents instead of 5. So the total he counted is 5N + 10N = 15N. But that total is 147 cents. So:\n",
            "\n",
            "15N = 147\n",
            "\n",
            "Let me solve for N:\n",
            "\n",
            "N = 147 / 15\n",
            "\n",
            "147 divided by 15. Let me do that. 15 times 9 is 135, so 147 minus 135 is 12. So N is 9.933..., but wait, the number of nickels must be a whole number. Hmm, maybe I made a mistake.\n",
            "\n",
            "Wait, let me check the problem again. The brother thought all nickels were dimes. So if he counted N nickels as dimes, then each nickel would be counted as 10 cents instead of 5. So the total he counted is 5N + 10N = 15N. But that total is 147 cents. So:\n",
            "\n",
            "15N = 147\n",
            "\n",
            "Dividing both sides by 15 gives N = 9.8. But that's not a whole number. So maybe the problem has a typo, or maybe I misinterpreted it.\n",
            "\n",
            "Wait, maybe the brother didn't count all nickels as dimes. Maybe he counted some nickels as dimes and some as nickels. But the problem says he mistook all nickels for dimes. So he must have counted all nickels as dimes. So the equation should be 5N (nickels as dimes) + 10N (dimes as dimes) = 147. So 15N = 147. Then N = 9.8. But that's not possible. So maybe the problem is wrong.\n",
            "\n",
            "Wait, let me check the total amount again. Jenifer has 82 cents. So N + P = 82. The brother counted 147 cents, which is 15N. So 15N = 147. Then N = 9.8. But since the number of nickels must be a whole number, maybe the problem is supposed to be 147 cents, which is 15 nickels. But then the total would be 15*5 + 15*10 = 75 + 150 = 225 cents, which is more than 82. So that doesn't work.\n",
            "\n",
            "Wait, maybe the brother counted some nickels as dimes and some as nickels. But the problem says he mistook all nickels for dimes. So he must have counted all nickels as dimes. So the equation is 5N + 10N = 15N = 147. Then N = 9.8. But that's not possible. So maybe the problem is wrong.\n",
            "\n",
            "Wait, maybe the brother counted all nickels as dimes, but the total is supposed to be 147 cents, which is 15 nickels. But then the total would be 15*5 + 15*10 = 75 + 150 = 225 cents, which is more than 82. So that doesn't work. So maybe the problem is supposed to be 147 cents, which is 15 nickels. But then the total would be 15*5 + 15\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    dataset[0][\"Messages\"][:2],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtZ3qGOALF95"
      },
      "source": [
        "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSZ0DET7bob",
        "outputId": "44c71c48-8518-4c04-9877-b38e5e6c041f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "del dataset\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "e13868e2b8b342acb1d1dac9bad36b23",
            "8721fc5178714799a1e16cf7bfe36d31",
            "e2356755ed6b4898a315f5b62e49d5bf",
            "d0c3e0c193514f10b8f63221a99d221c",
            "ed83b26c9be744ef83bb8569e5475cd7",
            "614f33069c59485c99d6dc5a5370f56f",
            "6e838e62789d46979cab90e85c4a7f74",
            "cd19f6e1fc0747049a3d0ad2659a81ad",
            "61fcd3b4b7ba4a16a3e197fa1e346caa",
            "33d094aae6a345e79e145bcd45108164",
            "a2ae13fef001462e89bc780dc40a0a39",
            "a7993e1dae6a4509b45edb399fe0fe6e",
            "af77c83cdbb3462e83e5846d2c7d7907",
            "a3410b6d3b114148b9808b6db1cd4b17",
            "7269ab3382af4ac699a3ffa933b152d2",
            "6fca825c104c4973a9f397e5fa0c21f7",
            "0bba12a7613b41749b162c624332e9f7",
            "d682db7c3016412ba857305efeab5b52",
            "d5c8ec58e71344c0b1fd42f311a8f763",
            "e09732c0d76c42928395f37a62342e32",
            "5cb4e7c57481488ab6906325e49a7e85",
            "1506c94c991c49b89f4e1081c1f4177a",
            "8e7b7b85fa3846c1a72fc5be043670aa",
            "d13dab1fcea84cc88c16a2617a7856c2",
            "d16f0dfef7b24a5cba715442de407d50",
            "e038d9ddb660496797a7579faad7931b",
            "ff092d18246748f5a4497bd57f4cd889",
            "d9c994403eb84556aaad81785dc487ee",
            "831e710726cb43eeb7e31852f29c2f73",
            "5646273504b84604a3f5ded1a18c4aff",
            "e478224a08a14d45bba1ed9ba075aece",
            "67fdca41df4d4e97bcf40e803d7bc03d",
            "913133021baf4f9eaae2216334699e35"
          ]
        },
        "id": "o7-eUrQn-OzE",
        "outputId": "332bc017-99ca-444b-a3d3-13392dbf1110"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e13868e2b8b342acb1d1dac9bad36b23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "en/train-00000-of-00001.parquet:   0%|          | 0.00/5.23M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7993e1dae6a4509b45edb399fe0fe6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/14116 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e7b7b85fa3846c1a72fc5be043670aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
              "    num_rows: 14116\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b00gUsS-ROW"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "siopxjG8-ReF",
        "outputId": "7f53852c-df11-4b2c-8ca8-dae724a7ead7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KGupRQqD-Wcf",
        "outputId": "eb2b337f-1080-4a70-bae5-d3c229f3c881"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'34'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "dataset[0][\"solution\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnXj6hn-Ydi"
      },
      "source": [
        "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8JJGXKdJ-Zl_",
        "outputId": "f9b795e2-78a3-4fce-f756-7e5451313114"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'34'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "def extract_hash_answer(text):\n",
        "    # if \"####\" not in text: return None\n",
        "    # return text.split(\"####\")[1].strip()\n",
        "    return text\n",
        "extract_hash_answer(dataset[0][\"solution\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30CygaU-dir"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394,
          "referenced_widgets": [
            "8639778fc0004279a76cffdd77619765",
            "215cfcb2a3eb4fc9b7902c4d9afe91e0",
            "c5c899c824264437a11cf0ed81e59958",
            "766fbb2b1bbf4fd1b2d9cc01d3ceacb4",
            "bede7de886714d8883977e8622cd47fc",
            "91bc15faed9a4d9299393dbcd52e3c58",
            "26c2ed00a1e447e88a8fc4dac63c2775",
            "11979ef4af6a470d908344c7020e442f",
            "00ee4e52f4b748c0835904117f674014",
            "2f1307018d7041f78b936dd644e36526",
            "3c472138a01c45d0bb07b802a3bf1456"
          ]
        },
        "id": "qyEVI972-d3n",
        "outputId": "aa2e4699-4607-495e-b86a-f3037c5b56bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14116 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8639778fc0004279a76cffdd77619765"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <think> and </think>.\\nThen, provide your solution between <answer></answer>',\n",
              "   'role': 'system'},\n",
              "  {'content': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.',\n",
              "   'role': 'user'}],\n",
              " 'solution': '34',\n",
              " 'data_source': 'math_dapo',\n",
              " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
              "   'role': 'user'}],\n",
              " 'ability': 'MATH',\n",
              " 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'},\n",
              " 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'},\n",
              " 'answer': '34'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m8eR9T-gMh"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQwjTjNz-gY_",
        "outputId": "13346f44-3ce3-4a4c-8ac4-23fcbe13c9ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r'</think>.*?<answer>(.+?)</answer>[\\s](0,)(?:<\\|endoftext\\|>)?[\\s]{0,}$',\n",
              "re.MULTILINE|re.DOTALL|re.UNICODE)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Add optional EOS token matching\n",
        "solution_end_regex = rf\"{solution_end}[\\s]{0,}\" + \\\n",
        "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycMneOq-iNC"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndzHnQ_6-jHt",
        "outputId": "c3c98e36-1ec5-41dd-8c36-d9192796ab32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    f\"Let me think!{reasoning_end}\"\\\n",
        "    f\"{solution_start}\\n2\\n{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRMDAzDk2x6t",
        "outputId": "32d80893-2445-48c4-bb9f-e70b2fee651e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    f\"{reasoning_start}Let me think!{reasoning_end}\"\\\n",
        "    f\"{solution_start}  2  {solution_end}\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOjmO5l-kl3"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "\n",
        "        # No need to reward <start_working_out> since we always prepend it!\n",
        "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAUWwtE-s6n"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hmtI_8gg-uIE"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        # Correct answer gets 5 points!\n",
        "        if guess == true_answer:\n",
        "            score += 5.0\n",
        "        # Match if spaces are seen, but less reward\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 3.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
        "                else: score -= 2.5 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 4.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atMyfhXh-v3R"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "\n",
        "We also remove possible commas for example as in 123,456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVW0kL8q-wL5",
        "outputId": "17b0e27f-fd72-4ccf-a302-e832c4d4a22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0.34']\n",
            "['123,456']\n",
            "['-0.234']\n",
            "['17']\n"
          ]
        }
      ],
      "source": [
        "match_numbers = re.compile(\n",
        "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "print(match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\"))\n",
        "print(match_numbers.findall(f\"{solution_start}  123,456  {solution_end}\"))\n",
        "print(match_numbers.findall(f\"{solution_start}  -0.234  {solution_end}\"))\n",
        "print(match_numbers.findall(f\"{solution_start}17{solution_end}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfaaAywNHHh"
      },
      "source": [
        "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GjBFrttr-y1_"
      },
      "outputs": [],
      "source": [
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Print only every few steps\n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "        print(\n",
        "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "    PRINTED_TIMES += 1\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(-2.5)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            # Remove commas like in 123,456\n",
        "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
        "            scores.append(3.5 if guess == true_answer else -1.5)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgOR3wJ_AyLr"
      },
      "source": [
        "Get the top 90% prompt length so we don't accidentally truncate them!\n",
        "\n",
        "Ie we'll remove the top 10% long prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "4044aa95a2414c02bb83f8ac17ebd03c",
            "12d92645a9f44063b17e06ee4573337b",
            "7c8d5fdf5212465691b792babd8489e2",
            "f251d57364cd42dfbf62fd4e0294a2c8",
            "4d19bc46991a406881a433fd6af3ec9e",
            "15999d7b09ea48cabe8eaff40d78f6c2",
            "1c16f16ac5b4425aa347e49d2439b59e",
            "cfb1e32521d0442d8e5f976a652204d5",
            "c8d31b2a43924a6d9af3a31badea7aa8",
            "8c997e8f0ffc48aeb431d1d06fd50cff",
            "9b6c9d8e84b44390b5791c0cb9ffcfe4",
            "0b128f6bd9b441dfb9ecac2bda23548c",
            "a27419f54130440f9f306cb57830a9c0",
            "e01fc14670784aebac6aa1a50f30f432",
            "e824618c951b4bb08a019ae77bb97956",
            "d132a412b7cb400095943f7587c0178d",
            "a9e31241999f4a6f8756a0c02b221172",
            "81f2815b04b146d79b8693338f936fc2",
            "3055147c72214b70bf440bc0002bf0c7",
            "9215322de5de4009a3d4ae104bd795ef",
            "85406bbe48df49629b4f2901203bf534",
            "c0009b32a6c74890aaa707349e5e07c6"
          ]
        },
        "id": "6EgAi4Q5fGE-",
        "outputId": "d3f53ba1-6af3-4a22-e41f-fad6359d23e9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14116 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4044aa95a2414c02bb83f8ac17ebd03c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <think> and </think>.\n",
            "Then, provide your solution between <answer></answer><|endoftext|>In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<think>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14116 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b128f6bd9b441dfb9ecac2bda23548c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length =  191\n"
          ]
        }
      ],
      "source": [
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
        "    batched = True,\n",
        ")\n",
        "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
        "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
        "\n",
        "import numpy as np\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(\"Max Length = \", maximum_length)\n",
        "\n",
        "# Filter only samples smaller than 90% max length\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "del tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "769030ad-e1d9-453a-c01b-d1ce2b34b7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 2\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 1e-5,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 2, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 20,\n",
        "    save_steps = 20,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "4b8538ea-c908-4931-b97a-7097519e2c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 12,728 | Num Epochs = 1 | Total steps = 20\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 17,432,576 of 1,700,000,000 (1.03% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************Question:\n",
            "In the diagram, each of the three identical circles touch the other two.  The circumference of each circle is 36.  What is the perimeter of the shaded region? [asy]\n",
            "\n",
            "defaultpen(1);\n",
            "\n",
            "path p = (1, 0){down}..{-dir(30)}dir(-60){dir(30)}..{dir(-30)}((2, 0) + dir(-120)){-dir(-30)}..{up}(1, 0)--cycle;\n",
            "fill(p, gray(0.75));\n",
            "\n",
            "draw(unitcircle);\n",
            "draw(shift(2 * dir(-60)) * unitcircle);\n",
            "draw(shift(2) * unitcircle);\n",
            "[/asy] \n",
            "Answer:\n",
            "18 \n",
            "Response:\n",
            "Okay, let's see. I need to find the perimeter of the shaded region in a diagram where three identical circles are touching each other. Each circle has a circumference of 36. So, first step: find the radius of each circle.\n",
            "\n",
            "Circumference formula is C = 2œÄr. Given C is 36, so 36 = 2œÄr. Dividing both sides by 2: r = 18/œÄ. Hmm, so each circle has a radius of 18/œÄ.\n",
            "\n",
            "Now, since each of these circles touches the two others, the centers of the circles form an equilateral triangle because all the circles are identical and touch each other symmetrically. So, the distance between any two center points is equal to the radius, so 18/œÄ.\n",
            "\n",
            "Wait, if each circle touches two others, then the distance between two opposite centers (which is actually the side length of the equilateral triangle) would be 2*(18/œÄ) = 36/œÄ. Because the circles are in the corners, and the straight line is twice the radius. That makes sense because 2*18/œÄ should give that distance between the centers across the center of the triangle.\n",
            "\n",
            "Wait, let me think again. If the three circles have radius 18/œÄ, then the distance between the centers of two touching circles is 36/œÄ. Each of these circles spans that distance between centers. So yeah, 36/œÄ is the side length of the larger equilateral triangle formed by the centers.\n",
            "\n",
            "So the perimeter of the shaded region would be the perimeter of this larger triangle. Since equilateral, each side length is 36/œÄ. So perimeter = 3 * (36/œÄ) = 108/œÄ.\n",
            "\n",
            "Wait, let me double-check this. The original circles have radius 18/œÄ, so the distance between opposite centers is 2*18/œÄ = 36/œÄ. Yes, so that's the side length of the triangle. That triangle has three sides of length 36/œÄ. So three times that is 108/œÄ. That seems right.\n",
            "\n",
            "But let me visualize again. If I take one circle in the lower left, its center is at x=0, y=-18/œÄ. The next one up is centered at x=18/œÄ, y=0, and the third is at x=18/œÄ, y=18/œÄ. The distance between the first and third centered circle is the partial width, which is 36/œÄ. But since the overlapping shaded region is part of the outer dials, it should be twice that distance because the triangles shared by two are outer parts.\n",
            "To solve the problem, we start by finding the radius \\( r \\) of the circles. Given that the circumference \\( C \\) is 36, we use the formula for the circumference of a circle:\n",
            "\n",
            "\\[\n",
            "C = 2\\pi r\n",
            "\\]\n",
            "\n",
            "Substituting \\( C = 36 \\):\n",
            "\n",
            "\\[\n",
            "36 = 2\\pi r\n",
            "\\]\n",
            "\n",
            "Solving for \\( r \\):\n",
            "\n",
            "\\[\n",
            "r = \\frac{36}{2\\pi} = \\frac{18}{\\pi}\n",
            "\\]\n",
            "\n",
            "Next, we determine the distance between the centers of two touching circles. Since the centers of the three circles form an equilateral triangle with each side equal to the diameter of one circle, the side length \\( s \\) of this triangle is:\n",
            "\n",
            "\\[\n",
            "s = 2r = 2 \\left( \\frac{18}{\\pi} \\right) = \\frac{36}{\\pi}\n",
            "\\]\n",
            "\n",
            "The perimeter \\( P \\) of the shaded region, which is the perimeter of the equilateral triangle formed by the centers, is given by:\n",
            "\n",
            "\\[\n",
            "P = 3 \\times s = 3 \\times \\frac{36}{\\pi} = \\frac{108}{\\pi}\n",
            "\\]\n",
            "\n",
            "Thus, the perimeter of the shaded region is:\n",
            "\n",
            "\\[\n",
            "\\boxed{\\frac{108}{\\pi}}\n",
            "\\] miƒôdzyn‡¥é‡¥∏‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µç‡µçÿ¨ŸÖ·Éê \n",
            "Extracted:\n",
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 28:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / match_format_exactly / mean</th>\n",
              "      <th>rewards / match_format_exactly / std</th>\n",
              "      <th>rewards / match_format_approximately / mean</th>\n",
              "      <th>rewards / match_format_approximately / std</th>\n",
              "      <th>rewards / check_answer / mean</th>\n",
              "      <th>rewards / check_answer / std</th>\n",
              "      <th>rewards / check_numbers / mean</th>\n",
              "      <th>rewards / check_numbers / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1163.500000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1326.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1163.500000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1326.000000</td>\n",
              "      <td>0.131135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-5.500000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130704</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.250000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.250000</td>\n",
              "      <td>1.767767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-3.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1406.000000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>1844.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1406.000000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>1844.000000</td>\n",
              "      <td>0.160388</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184540</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-5.500000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>1719.000000</td>\n",
              "      <td>1582.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1582.000000</td>\n",
              "      <td>1582.000000</td>\n",
              "      <td>1582.000000</td>\n",
              "      <td>0.233107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-5.500000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>1660.500000</td>\n",
              "      <td>1465.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1465.000000</td>\n",
              "      <td>1465.000000</td>\n",
              "      <td>1465.000000</td>\n",
              "      <td>0.169208</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-5.500000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>1331.500000</td>\n",
              "      <td>1078.000000</td>\n",
              "      <td>1585.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1331.500000</td>\n",
              "      <td>1078.000000</td>\n",
              "      <td>1585.000000</td>\n",
              "      <td>0.143627</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.158415</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.118573</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.116224</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>6.363961</td>\n",
              "      <td>1440.500000</td>\n",
              "      <td>1081.000000</td>\n",
              "      <td>1800.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1440.500000</td>\n",
              "      <td>1081.000000</td>\n",
              "      <td>1800.000000</td>\n",
              "      <td>0.151130</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>4.242640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>3.535534</td>\n",
              "      <td>1528.500000</td>\n",
              "      <td>1437.000000</td>\n",
              "      <td>1620.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1528.500000</td>\n",
              "      <td>1437.000000</td>\n",
              "      <td>1620.000000</td>\n",
              "      <td>0.158328</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.535534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1376.500000</td>\n",
              "      <td>897.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>897.000000</td>\n",
              "      <td>897.000000</td>\n",
              "      <td>897.000000</td>\n",
              "      <td>0.097340</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-5.500000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>1481.000000</td>\n",
              "      <td>1106.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1106.000000</td>\n",
              "      <td>1106.000000</td>\n",
              "      <td>1106.000000</td>\n",
              "      <td>0.173257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1337.000000</td>\n",
              "      <td>818.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>818.000000</td>\n",
              "      <td>818.000000</td>\n",
              "      <td>818.000000</td>\n",
              "      <td>0.043660</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-4.750000</td>\n",
              "      <td>3.889087</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100374</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.121320</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.250000</td>\n",
              "      <td>1.767767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-3.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>654.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>954.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>654.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>954.000000</td>\n",
              "      <td>0.182482</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************Question:\n",
            "In a class of 20 students, all but 4 of the students put their names on a typed assignment. If the teacher randomly guesses, what is the probability that she correctly guesses which paper belongs to each of the four remaining students? Express your answer as a common fraction.The answer is in the form \frac{m}{n}, where gcd(m, n) = 1. Please provide the value of m + n. \n",
            "Answer:\n",
            "25 \n",
            "Response:\n",
            "Okay, so I need to find the probability that the teacher randomly guesses the correct paper for each of the four remaining students. Hmm, let's see.\n",
            "\n",
            "First, let's figure out how many papers are there in total. If there are 20 students and only 4 don't have their names on the paper, then the rest must have their names. So 20 minus 4 is 16 papers in total.\n",
            "\n",
            "Now, when the teacher randomly guesses, what's the chance she gets the first paper right? There are 16 papers to choose from, so the probability is 1 out of 16, or 1/16.\n",
            "\n",
            "But wait, after she guesses one correctly, then there are 15 papers left for the next student. But since the teacher is guessing independently for each, does that mean the previous guesses don't affect the ones that follow? Like, the first one being right doesn't influence the second's guess? If so, then the probabilities stay the same.\n",
            "\n",
            "This is like the concept of independent events. For example, if you flip two coins, the outcome of the first flip doesn't affect the second. Similarly here, each guess is independent. So each time she guesses, the probability is always 1/16.\n",
            "\n",
            "Therefore, the total probability of guessing all four correctly would be the product of the probabilities of each guess being right. So (1/16) multiplied four times itself, which is (1/16)^4.\n",
            "\n",
            "Wait, but let me check again. If there are 16 papers, first one has 16 options, second one has 15 options left, then 16/16, then 15/16, and so on. But since the teacher is guessing independently each time, it's just 1/16 * 1/16 * 1/16 * 1/16, which is 1/16^4.\n",
            "\n",
            "Calculating 16^4, that's 65,536. But the probability should be 1 divided by that, right? Because probability of independent events being right is the product of their individual probabilities. So 1/65,536.\n",
            "\n",
            "But the problem asks for the answer in the form 1/2^n, where n is an integer. Well, 65,536 is 2 raised to the power of 16 (since 2^16 = 65,536). So that definitely fits in the form 1/2^16, meaning m is 1 and n is 16.\n",
            "\n",
            "So then, m + n would be 1 + 16 = 17. But wait, let me verify again. If the teacher has 16 papers to guess from, and each guess is 1/16, then yes, 1/65,536. Right. So m is 1, n is 16. Adding those gives 17. So the final answer should be 17.\n",
            "To find the probability that the teacher randomly guesses the correct paper for each of the four remaining students, we need to calculate the product of the probabilities of each individual guess being correct.\n",
            "\n",
            "1. There are \\(20\\) students, and \\(4\\) do not have their names on the paper. Therefore, there are \\(16\\) papers in total that need to be guessed correctly.\n",
            "\n",
            "2. The probability that the first paper is guessed correctly is \\(\\frac{1}{16}\\).\n",
            "\n",
            "3. After the first guess is correct, there are \\(15\\) papers left for the second guess, so the probability is \\(\\frac{15}{16}\\).\n",
            "\n",
            "4. After the second guess is correct, there are \\(14\\) papers left for the third guess, so the probability is \\(\\frac{14}{15}\\).\n",
            "\n",
            "5. After the third guess is correct, there are \\(13\\) papers left for the fourth guess, so the probability is \\(\\frac{13}{14}\\).\n",
            "\n",
            "6. The overall probability of all four guesses being correct is the product of these individual probabilities:\n",
            "   \\[\n",
            "   \\frac{1}{16} \\times \\frac{15}{16} \\times \\frac{14}{15} \\times \\frac{13}{14}\n",
            "   \\]\n",
            "\n",
            "7. We can simplify this by canceling out the common terms:\n",
            "   \\[\n",
            "   \\frac{1}{16} \\times \\frac{15}{16} \\times \\frac{14}{15} \\times \\frac{13}{14} = \\frac{1 \\times 15 \\times 14 \\times 13}{16 \\times 15 \\times 14 \\times 13} = \\frac{13}{16 \\times 13} = \\frac{1}{16}\n",
            "   \\]\n",
            "\n",
            "Thus, the probability that the teacher correctly guesses all four papers is \\(\\frac{1}{16}\\). In the form \\(\\frac{m}{n}\\), where \\(\\gcd(m, n) = 1\\), we have \\(m = 1\\) and \\(n = 16\\).\n",
            "\n",
            "Therefore, the value of \\(m + n\\) is:\n",
            "\\[\n",
            "\\boxed{17}\n",
            "\\]ÂüÉÂèäÊú®‰πÉ‰ºäÁâåÂè´Â§öÂ∞ë\n",
            "\n",
            "Okay, so I remember the problem is about Egyptian numerals, and Egyptians had this number system where they used distinct symbols for powers of 10. Let's see if I can recall how many cards they had in a specific game, called Senet.\n",
            "\n",
            "Wait, in Senet, there's a chapter that mentions Egyptian symbols and the number of cards. Let me check the book here. According to the book, Senet involves 3 dots on each card, which is 45 per group of 15 cards, totaling 135 cards. But the problem is asking for how many Egyptian numerals there are.\n",
            "\n",
            "Hmm, so Egyptian numerals are based on powers of 10. For example, the number 2 is represented by two strokes, 5 by a raised stroke, and 10 by a heel-shaped hieroglyph. So the sequence would be 1, 10, 100, 1000, and so on.\n",
            "\n",
            "For the card problem, let's assume Egyptian numerals are represented by names on cards, not the symbols themselves. So if they have names like 'twenty-three' or 'three hundred ten,' then each card would have one of those names written on it.\n",
            "\n",
            "According to the book, there were 30,476 cards in total for Senet. To find the number of Egyptian numerals, we divide the total number of cards by the average number of symbols per name. Wait, but the book might have given a different number. Maybe I have to check that.\n",
            "\n",
            "Let's locate the exact quote in the book. Ah, in Chapter 3 on Counting Arts and Crafts, they mention that Egyptian numeral names were on the cards. For example, in a tentil (ten cards per group of 15), there were 630 Egyptian numeral names. Hmm, so for one tentil (10 cards), there would be 63 cards with such names.\n",
            "\n",
            "Since a Senet game has 4251 cards (I think that's correct from the book), we divide 4251 by 63 to find the number of tentils. So 4251 divided by 63 is approximately 67. So each tentil has 63 cards with Egyptian numeral names. Therefore, the total number of these Egyptian numeral names is 67 times 63, which is 4221.\n",
            "\n",
            "So then, the number of Egyptian numerals is 4221. But wait, let's confirm the division again. 63 * 60 is 3780, 63 * 3 is 189, so 4221. Yep, that's right. Then the answer should be 4221. But looking at the problem again, is the book's figure correct? Yeah, maybe they meant 10 tiles in a group of 15, not all 7108. Wait, 10*15*63 is 9450 minus 516 to get 4221. But 63*63 is 4069, right? So 7108 divided by 10*63 is approximately 10.1. So the exact number probably splits neatly.\n",
            "\n",
            "But the problem asks for 'how many,' implying a whole number. So 8 tentils? Wait,  \n",
            "Extracted:\n",
            "None\n",
            "********************Question:\n",
            "Find the minimum possible value of\n",
            "\\[\\sqrt{58 - 42x} + \\sqrt{149 - 140 \\sqrt{1 - x^2}}\\]where $-1 \\le x \\le 1.$The answer is in the form k\\sqrt{m}+n,. Please provide the value of k + m + n. \n",
            "Answer:\n",
            "110 \n",
            "Response:\n",
            "Okay, let me try to tackle this problem. I need to find the minimum value of this expression involving square roots:\n",
            "\n",
            "\\[ \\sqrt{58 - 42x} + \\sqrt{149 - 140\\sqrt{1 - x^2}} \\]\n",
            "\n",
            "For all \\( x \\) in the interval \\([-1, 1]\\). The desired minimum value should be in the form \\( k\\sqrt{m} + n \\), and I need to find \\( k + m + n \\).\n",
            "\n",
            "Hmm, the expression is a sum of two square roots. Maybe I can use a substitution to simplify it. Since the second square root has a term like \\( 140\\sqrt{1 - x^2} \\), I recall that \\(\\sqrt{1 - X^2}\\) is related to the sine function. Let me set \\( x = \\sin{\\theta} \\), because that would make the expressions trigonometric. Then \\( x \\) goes from \\(-1\\) to \\(1\\), which are the bounds of \\(\\theta\\) in radians. So, \\( \\theta \\) is in \\([-œÄ/2, œÄ/2]\\).\n",
            "\n",
            "Let's try that substitution. Let \\( x = \\sin\\theta \\). Then the expression becomes:\n",
            "\n",
            "\\[ \\sqrt{58 - 42\\sin\\theta} + \\sqrt{149 - 140\\sqrt{1 - \\sin^2\\theta}} \\]\n",
            "\n",
            "Now, I recall that \\( \\sqrt{1 - \\sin^2\\theta} = \\cos\\theta \\) (since \\( \\theta \\) is in \\([-œÄ/2, œÄ/2]\\), so cosine is non-negative in that range). So, the second square root simplifies to:\n",
            "\n",
            "\\[ \\sqrt{149 - 140\\cos\\theta} \\]\n",
            "\n",
            "Now the expression is:\n",
            "\n",
            "\\[ \\sqrt{58 - 42\\sin\\theta} + \\sqrt{149 - 140\\cos\\theta} \\]\n",
            "\n",
            "This still looks complicated. Maybe I can combine these into a single square root. I have a sum of square roots, and I need to find the minimum. The minimum of a sum of square roots occurs when the individual roots are equal. Wait, but they have different coefficients. Hmm, perhaps I need to find the minimum of each part separately and then balance them.\n",
            "\n",
            "Let me denote \\( f(\\theta) = \\sqrt{58 - 42\\sin\\theta} + \\sqrt{149 - 140\\cos\\theta} \\). To find its minimum, I can first find the derivative and set it to zero.\n",
            "\n",
            "So, let's compute the derivative of \\( f(\\theta) \\):\n",
            "\n",
            "\\[ f'(\\theta) = \\frac{1}{2}(58 - 42\\sin\\theta)^{-1/2} \\cdot (-42\\cos \\theta) + \\frac{1}{2}(149 - 140\\cos\\theta)^{-1/2} \\cdot (140\\sin \\theta) \\]\n",
            "\n",
            "Simplifying the derivative:\n",
            "\n",
            "\\[ f'(\\theta) = \\frac{-21(42\\cos\\theta + 140\\sin\\theta)}{\\sqrt{(58 - 42\\sin\\theta)(149 - 140\\cos\\theta)}} \\]\n",
            "\n",
            "Setting \\( f'(\\theta) = 0 \\):\n",
            "\n",
            "\\[ -21(42\\cos\\theta + 140\\sin\\theta) = 0 \\]\n",
            "\n",
            "Dividing by -21:\n",
            "\n",
            "\\[ 42\\cos\\theta + 140\\sin\\theta = 0 \\]\n",
            "\n",
            "Simplify this equation. Divide everything by 14:\n",
            "\n",
            "\\[ 3\\cos\\theta + 10\\sin\\theta = 0 \\]\n",
            "\n",
            "Rearrange terms:\n",
            "\n",
            "\\[ 10\\sin\\theta = -3\\cos\\theta \\]\n",
            "\n",
            "Divide by \\(\\cos\\theta\\) (assuming \\(\\cos\\theta \\neq 0\\)):\n",
            "\n",
            "\\[ 10\\tan\\theta = -3 \\]\n",
            "\n",
            "So:\n",
            "\n",
            "\\[ \\tan\\theta = -\\frac{3}{10} \\]\n",
            "\n",
            "Then:\n",
            "\n",
            "\\[ \\theta = \\arctan\\left(-\\dfrac{3}{10}\\right) \\]\n",
            "\n",
            "Since \\( x = \\sin \\theta \\), we can find \\( x \\). The sine of an angle minus œÄ/2 is the same as the negative sine of the original angle. So the angle \\( \\theta \\) can be \\( \\arctan\\left(-\\dfrac{3}{10}\\right) - \\dfrac{\\pi}{2} \\).\n",
            "\n",
            "Thus:\n",
            "\n",
            "\\[ x = \\sin\\left(\\arctan\\left(-\\dfrac{3}{10}\\right) - \\dfrac{\\pi}{2}\\right) \\]\n",
            "\n",
            "But we recall that \\( \\sin(A - B) = \\sin A \\cos B - \\cos A \\sin B \\), so:\n",
            "\n",
            "\\[ x = \\sin\\left(\\arctan\\left(-\\dfrac{3}{10}\\right)\\right) \\cos \\left(\\dfrac{\\pi}{2}\\right) - \\cos\\left(\\arctan\\left(-\\dfrac{3}{10}\\right)\\right) \\sin\\left(\\dfrac{\\pi}{2}\\right) \\]\n",
            "\n",
            "Since \\( \\cos\\left(\\dfrac{\\pi}{2}\\right) = 0 \\) and \\( \\sin\\left(\\dfrac{\\pi}{2}\\right) = 1 \\), this simplifies to:\n",
            "\n",
            "\\[ x = -\\cos\\left(\\arctan\\left(-\\dfrac{3}{10}\\right)\\right) \\]\n",
            "\n",
            "Let \\( \\alpha = \\arctan\\left(-\\dfrac{3}{10}\\right) \\), so \\( \\tan\\alpha = -\\dfrac{3}{10} \\). Then \\( \\cos\\alpha = \\frac{10}{13} \\), since \\( \\tan\\alpha = \\dfrac{\\sin\\alpha}{\\cos\\alpha} \\) and \\( \\cos^2\\alpha + \\sin^2\\alpha = 1 \\).\n",
            "\n",
            "Thus:\n",
            "\n",
            "\\[ \\cos \\alpha = \\dfrac{10}{13} \\]\n",
            "\n",
            "So \\( x = -\\dfrac{10}{13} \\).\n",
            "\n",
            "Now let's check if this value works in the original expression. Calculate the expression at \\( x = -\\dfrac{10}{13} \\):\n",
            "\n",
            "First part: \\( \\sqrt{58 - 42(-\\dfrac{10}{13})} \\).\n",
            "\n",
            "Compute this:\n",
            "\n",
            "\\[ 58 - 42 \\times \\left(-\\dfrac{10}{13}\\right) = 58 + 42 \\times \\dfrac{10}{13} = 58 + \\dfrac{420}{13} = \\dfrac{58 \\times 13 + 420}{13} = \\dfrac{754}{13} \\]\n",
            "\n",
            "So the first square root is \\( \\sqrt{\\dfrac{754}{13}} = \\dfrac{\\sqrt{754}}{\\sqrt{13}} \\)\n",
            "\n",
            "Second part: \\( 149 - 140 \\times \\sqrt{1 - \\left(-\\dfrac{10}{13}\\right)^2} \\)\n",
            "\n",
            "Compute \\( \\left(-\\dfrac{10}{13}\\right)^2 = \\dfrac{100}{169} \\), so \\( 1 - \\dfrac{100}{169} = \\dfrac{69}{169} \\). Thus, \\( \\sqrt{\\dfrac{69}{169}} = \\dfrac{\\sqrt{69}}{13} \\).\n",
            "\n",
            "Multiply by 140: \\( 140 \\times \\dfrac{\\sqrt{69}}{13} = \\dfrac{140\\sqrt{69}}{13} \\). Then subtract from 149:\n",
            "\n",
            "\\[ 149 - \\dfrac{140\\sqrt{69}}{13} = \\dfrac{149 \\times 13 - 140\\sqrt{69}}{13} = \\dfrac{1937 - 140\\sqrt{69}}{13} \\]\n",
            "\n",
            "So the second square root is \\( \\sqrt{\\dfrac{1937 - \n",
            "Extracted:\n",
            "None\n",
            "********************Question:\n",
            "Find the number of three-element subsets of $\\{1, 2, 3,...,13\\}$ that contain at least one element that is a multiple of $2$, at least one element that is a multiple of $3$, and at least one element that is a multiple of $5$ such as $\\{2,3, 5\\}$ or $\\{6, 10,13\\}$. \n",
            "Answer:\n",
            "63 \n",
            "Response:\n",
            "Yes, let's find the number of three-element subsets of $\\{1, 2, \\ldots, 13\\}$ that contain at least one element that is a multiple of $2$, at least one element that is a multiple of $3$, and at least one element that is a multiple of $5$.\n",
            "\n",
            "To start, let's identify the relevant numbers within the set $\\{1, 2, \\ldots, 13\\}$:\n",
            "- Multiples of 2: $\\{2, 4, 6, 8, 10, 12\\}$\n",
            "- Multiples of 3: $\\{3, 6, 9, 12\\}$\n",
            "- Multiples of 5: $\\{5, 10, 15\\} \\subseteq \\{1, 2, \\ldots, 13\\}$*\n",
            "\n",
            "Here, we notice that there is a few issues:\n",
            "1. The set of multiples of 5 isn't entirely within $\\{1, 2, \\ldots, 13\\}$, thus requiring a check and adjustment in the problem statement.\n",
            "2. The subset should include at least one element from each group (multiples of 2, 3, and now needing to be adjusted to fit in the correct grouping).\n",
            "\n",
            "Let's inspect each group:\n",
            "- Multiples of 2: 6 elements (2, 4, 6, 8, 10, 12).\n",
            "- Multiples of 3: 4 elements (3, 6, 9, 12).\n",
            "- Multiples of 5: 1 element (5).\n",
            "\n",
            "Now, we need to determine the number of three-element subsets that contain at least one element from each group. This can be tackled using the principle of inclusion-exclusion:\n",
            "- Total number of three-element subsets from six elements (multiples of 2: 6 elements).\n",
            "- Subtract subsets that don't meet the condition (none, as they have to be chosen from their intersection).\n",
            "\n",
            "First, find the total number of three-element subsets from 6 elements:\n",
            "\\[\n",
            "\\binom{6}{3} = \\frac{6 \\times 5 \\times 4}{1 \\times 2 \\times 3} = 20\n",
            "\\]\n",
            "\n",
            "Next, calculate the subsets that don't contain a multiple of 2. Each such subset must be chosen from the 4 elements that are multiples of 3:\n",
            "\\[\n",
            "\\binom{4}{3} = 4\n",
            "\\]\n",
            "\n",
            "So, the number of subsets that do not contain a multiple of 2 is 4. Since these subsets are not valid (they don't meet the condition of having at least one multiple of 2), we subtract this from the total:\n",
            "\\[\n",
            "20 - 4 = 16\n",
            "\\]\n",
            "\n",
            "However, we must ensure that these 16 subsets still avoid having at least one multiple of 3 upon subtracting. This condition is inherently included in the total of 16, so this subtraction is valid without adding any new invalid sets.\n",
            "\n",
            "Therefore, the number of three-element subsets that contain at least one multiple of each is:\n",
            "\\[\n",
            "\\boxed{16}\n",
            "\\]ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿ®ŸÑÿ∑ŸÅ Ÿàÿ®ŸÜÿßÿ° ÿπŸÑŸâ ÿßŸÑÿØŸÇÿ© ÿßŸÑÿ™ÿπŸÑŸäŸÖŸäÿ©. ÿ≥ÿ™ŸÉŸàŸÜ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÅŸáÿ±ÿ≥ÿ™ ŸÇŸàŸä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ£ŸÖÿ´ŸÑÿ© ŸÖÿ±Ÿàÿ±Ÿäÿ©  ŸàŸÖÿ™ÿπÿØÿØ ÿßŸÑŸÖŸÇÿßŸäŸäÿ≥ Ÿäÿ™ÿ∂ŸÖŸÜ: ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ Ÿàÿ´ŸÜÿßÿ¶Ÿäÿ© ŸàÿßŸÜÿ™ŸÇÿßŸÑÿßÿ™ ÿßŸÑÿ™ÿßÿ±ŸäÿÆŸäÿ©. ÿ™ÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑŸÖÿ®ÿ±ŸÖÿ¨ ŸÑÿßÿ®ÿØÿßÿ° ÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ≠ŸÑŸàŸÑ Ÿàÿßÿ≠ÿØÿ© Ÿàÿßÿ≠ÿØÿ© ŸÑÿ£ŸÖÿ´ÿßŸÑŸäÿ™ŸäŸÜ ŸàŸÜÿ¥Ÿàÿ° ÿÆŸàÿµŸäÿ© ÿßŸÑÿØÿ±ÿ¨ÿ© ÿ®ÿßŸÑÿ®ŸÜÿßÿ° ÿßŸÑŸÖÿ™Ÿâÿ≥ÿ© –æ–±—É—á–µ–Ω–∏—è ÿ£ŸÜŸá Ÿäÿ¨ÿ® ÿ±ÿ®ÿ∑ ÿßŸÑÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© ŸàÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ÿßŸÑŸÖÿ™ŸÉÿßŸÖŸÑÿ© ŸÉŸÖÿ≠ÿßŸàŸÑÿ© ŸÜÿ¨ÿßÿ≠ ÿ£ÿØŸÑÿ©. ÿ≥ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ŸÇÿßŸÜŸäÿ™Ÿäÿ±Ÿä ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä  Ÿàÿ±ŸÖŸàÿ≤ ŸÖÿπÿ®ÿ±ÿßÿ™ ŸÖÿØÿ©  ÿ≥ÿßÿπÿ© until —Ä–µ—à–∞. ÿßŸÑÿ¨ÿ±ÿπÿßÿ™ ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ©  ŸÖŸÜ ÿßŸÑŸÖÿ®ÿ±ŸÖÿ¨ ŸÑÿØŸäŸáÿß 60. ÿ™ÿ™ÿ∂ŸÖŸÜ hacia problemas combinatoriales combinatorial. ÿ≥ÿ™ŸÉŸàŸÜ ÿ£ŸÅÿßŸÇ ŸÑÿ∫ŸàŸä  Ÿàÿ´ŸÜÿßÿ¶Ÿäÿ©. ŸäŸÇÿßÿ®ŸÑÿßŸÑŸÖACHER: ŸÜÿπŸÖÿå ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿ™ÿ±ŸäÿØ ÿ£ŸÜ ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÅŸáÿ±Ÿëÿ≥ÿ™ ŸÇŸàŸä. ŸäŸÉÿ™ÿ® ŸÜŸÅÿ≥Ÿá ÿßŸÑŸÖÿ≥ŸáŸÑ  ÿ•ŸÑŸâ  underdelicate.Assisting teacher: Yes, the problem should have a strong list. Write it underdelicate. \n",
            "Extracted:\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20, training_loss=0.00013634944371005987, metrics={'train_runtime': 1752.7697, 'train_samples_per_second': 0.023, 'train_steps_per_second': 0.011, 'total_flos': 0.0, 'train_loss': 0.00013634944371005987})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "8d03bb3856be487b99c2903a91cd5177",
            "d9fe0150dc2946cf994a6a21547acbd9",
            "dbe05ad284444e029cfc863e85c41b6e",
            "5165bf9b70604c39b31d5b10ef5485ef",
            "491f3e2b9ddb4a209398c455a26f9690",
            "e514bdc68d1b490a931b306f4facc0ac",
            "44f3b417a33740258f8441e94dc56405",
            "973516dd21944bc596b55e4f4cef541b",
            "3c89c2e0c5da458991c3252e619f253d",
            "97c3a972318e465291256f7a6e887634",
            "7e5b0c75e2d745e98f127b4456d18ed1"
          ]
        },
        "id": "qtcz_lpbVC92",
        "outputId": "3c6a68b2-60a9-4db5-84c7-851b66a2e76b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d03bb3856be487b99c2903a91cd5177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" - Answers\\nMath and Arithmetic\\nWhat is the sqrt of 101?\\nWiki User\\n‚àô 2010-12-09 11:20:10\\nBest Answer\\nCopy\\nsqrt(101) = 10.049875621106562.\\nThe principal (common) square root of 101 is 10.\\nThe only integer square root of 101 (the only integer whose\\nsquare is 101) is 10.\\nThe square root of 101 is a complex number.\\n(sqrt(101))^2 = 101. You can calculate it with a calculator or\\nby a method known as the long division algorithm.\\nIf you must know it to within 0.001 and you must give a rounded\\nanswer, then\\nsqrt(101) = 10.05, or\\nit's 10.05 plus some imaginary portion which you would know if\\nyou would just do a straight calculation and you have 7 decimal\\nplaces of precision, but you only round to the fourth decimal, so\\n\\nYou ask for:\\nsqrt(101) = 10.04 and when you add the imaginary portion to 10.04\\nyou will have your answer.\\nThe only integer square root of 101 that is a complex number is\\nthe only possible square root is 1004/10000 + 004i/10000.\\nThe square root of 1.\\nThe square root of 2.\\nThe square root of 4.\\nThe square root of 6.\\nThe square root of 10.\\nThe square root of 12\\nThe square root of 14.\\nThe square root of 16.\\nThe square root of 18.\\nThe square root of 20.\\nThe square root of 22.\\nThe square root of 400, or sqrt(100) = 10.\\nThe square root of 3 = 1.732.\\nThe square root of 3600 = 60.\\nThe square root of 101 is about 10.05.\\n10^1.005 = 10.0498.\\nThe square root of 101 is also an irrational number. Thus, there\\nis no integer (whole number) to root the number.\\nThere is an infinite number of irrational numbers between 10 and\\n11.\\nThese are 10.000000001... (and 10.01...). In theory these are\\nstill rational numbers - just irrational numbers that are\\nirrational to 10-digit standard decimal notation.\\nThe square root of 100 is the principal square root of the\\nnumber.  It is either 10 or -10. Either number can be square\\nrooted because each number times itself is 100.\\n10 x 10 = 100; same as -10 x -10 = 100.\\nThe square root of 100 is 10 or -10.\\nThe square root of 36 is -6 or 6.\\nThe square root of 48.\\nThe square root of 50.\\nThe square root of 320.\\nThe square root of 25 is 5 (and the opposite, -5) so 25 has a\\nroot of either 5 or -5.\\n5 squared is 25 and -5 squared is also 25; either number can be\\nthe square root of 25.\\nWe can take the square roots of 0, 1, 4, 9, 16, 25, 36, 49, 64,\\n81, 100...\\nSquaring any number which has this sequence as their digits\\nresults in the same sequence repeating.\\nExamples.\\n00 squared = 00 (000 squared = 000 (and so on)).\\n1 squared = 01 (11 squared = 121 (and so on)).\\n2 squared = 04 (22 squared = 484 (and so on)).\\n3 squared = 09 (33 squared = 1089...\\n4 squared = 16 (44 squared = 1936 (and so on).\\n5 squared = 25 (55 squared = 3025...\\n6 squared = 36 (66 squared = 4356...\\n7 squared = 49 (77 squared = 5929...\\n8 squared = 6\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "text = \"What is the sqrt of 101?\"\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558,
          "referenced_widgets": [
            "3cf71211397d417488cf05d219027732",
            "374ef60446054831933b1c02e79ea51d",
            "a1a2658f45bf43fd890429d1f124bcf3",
            "f6eb3c95316c4e6180938ab6a208449f",
            "4d589b124fc24aa5a2753c27bf9e31d5",
            "375635be6fe34dbfb990eafaa7e3b49c",
            "a7b6bd2dfecc44039ab7b14fd2f5ffe5",
            "b97a564f80b44de8ab65f5f5f271c143",
            "f637dc9d1c3a464cba9acfd1aac9369e",
            "53b353cc48ab498c8e9f5f5d9ddbce01",
            "53df2796a5374d7eaf2149574c6668e4"
          ]
        },
        "id": "zf_OY5WMVOxF",
        "outputId": "64d9468d-2305-4a7f-a7d6-beadce7a6f31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cf71211397d417488cf05d219027732"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, let's see. The problem is asking for the square root of 101. Hmm, square root. So, I need to find a number that when multiplied by itself equals 101. I remember that square roots are pretty exact, and sometimes for numbers close to perfect squares, their square roots would be a bit less than the square root of the nearby perfect squares.\\n\\nLet me think, the perfect squares around 101 are 81 and 100. 81 is 9^2, and 100 is 10^2. Since 101 is slightly more than 100 but less than 81, its square root should be between 9.5 and 9.999..., right? To be precise, because 9.9999... squared is 99.9, which is less than 100. But 9 squared is 81. So it's more than 9. Let me do a quick approximation.\\n\\nIf 9.5 squared is 90.25, which is too low, maybe 9.51 squared would be 90.4881, way too low. 9.52 squared is 90.7504. Wait, let's see: 9.4 squared is 88.36, which is too low. 9.45 squared? 90 plus 1.125 is 91.125, which is higher. So 9.45 is between 90.25 and 91.125. Hmm, that's 9.45 is between 90 and 91. So 101 is less than 101.225, since 9.51 squared is 90.4881 < 101, then 9.51 + 0.01 would make 9.52, 9.52^2 is 91.1296, which is higher than 101. So between 9.45 and 9.52. But the problem doesn't require super precision. Maybe 9.5 or 9.45.\\n\\nWait, 9.45^2 is 89.3025. Let‚Äôs check: 9.44^2 is 89.1796, so 9.45 slightly higher, 9.45 squared is 90.25625. 90.25625 + 0.75? 91.00625. So between 90 and 91. That's less than 101, but then we check 9.44^2 is 89.1796, then 9.445^2 is 89.300025. So 89.300025 + 0.7? 89.300025 + 0.7 = 89.370025. But 101 is way higher than that. So that's not it. I think I‚Äôm missing something here. Maybe the square root of 101 is 10.049875. Let me check 10^2 is 100, so higher. Then 10.1^2 is 102.01. So 10.1 is too high. Hmm, closer to 10.05. Let‚Äôs try 10.05^2. 10.05 squared is 101.0025. So that would be 101.0025. That's very close to 101. So the square root should be 10.05.\\n\\nWait, but 10.05^2 is 101.0025, which is 0.25 above 101. So the number needs to be a little bit higher than 10.05. But when I rounded 10.05 as the answer, let me check again. So 9.95^2 is 98.0025, which is lower than 99. Then 9.98^2 is 99.8004. So 9.98^2 is closer to the target, but still far from 101. 9.95 is the square of 98.025, so 98.025. Then 9.95^2 is 98, so 0.025. 9.95^2 is 98.025, so that's not it. Wait, maybe I made a mistake with rounding? Let me check the calculation again. 9.98^2 is 99.8004, which is too high, so 10.05, then 10.05^2 is 101.0025, which is 101+25 = 102. So that's one difference. For 9.98, 99.8004. So 9.98, then 99.804 + 0.0004? 99.80+. So 9.98, 99.804, then 99.804 + 0.0004 is 99.804 + 0.0004 = 99.8044. So 9.98^2 is 99.60064. So 9.98^2 is 99.60064, 99.804. That's 0.1836 above the target. So 9.98^2 is too low. Then 10.05^2 is 101.0025, which is close. So the answer should be 10.05. Right?\\nTo find the square root of 101, we can use a method that involves iterative approximation. Here are the steps we can follow:\\n\\n1. **Identify the nearest perfect squares:**\\n   - The perfect square just below 101 is \\\\(9^2 = 81\\\\).\\n   - The perfect square just above 101 is \\\\(10^2 = 100\\\\).\\n\\n2. **Estimate the square root:**\\n   - Since 101 is slightly more than 100, the square root must be a number slightly more than the square root of 100, which is 10.\\n   - Let's denote the square root of 101 by \\\\( \\\\sqrt{101} \\\\approx 10.0.49 \\\\).\\n\\n3. **Refine the estimate:**\\n   - Start with an initial guess of \\\\(10.5\\\\) (since \\\\(10^2 = 100\\\\) and \\\\(11^2 = 121\\\\), so somewhere between 10.5 and 10.5 will be the correct value).\\n\\n4. **Calculate \\\\(10.5^2\\\\):**\\n   - \\\\(10.5^2 = (10 + 0.5)^2 = 10^2 + 2 \\\\cdot 10 \\\\cdot 0.5 + 0.5^2 = 100 + 10 + 0.25 = 110.25\\\\).\\n\\n5. **Adjust the estimate:**\\n   - Since \\\\(110.25\\\\) is slightly above 101, the root must be slightly less than 10.5. Let's try 10.4.\\n\\n6. **Calculate \\\\(10.4^2\\\\):**\\n   - \\\\(10.4^2 = (10 + 0.4)^2 = 10^2 + 2 \\\\cdot 10 \\\\cdot 0.4 + 0.4^2 = 100 + 8 + 0.16 = 108.16\\\\).\\n\\n7. **Refine further:**\\n   - Since \\\\(10.4^2 = 108.16\\\\) is still above 101, try \\\\(10.1\\\\).\\n   - \\\\(10.1^2 = (10 + 0.1)^2 = 10^2 + 2 \\\\cdot 10 \\\\cdot 0.1 + 0.1^2 = 100 + 2 + 0.01 = 102.01\\\\).\\n\\n8. **Refine even more:**\\n   - Since \\\\(10.1^2 = 102.01\\\\) is slightly above, try a smaller increment, like 10.15.\\n   - \\\\(10.15^2 = (10 + 0.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QyEjW-WuYQIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "24aad7ae8540467991f70365038b8a18",
            "85f21060fda64ee2951a30966aa17b3c",
            "61d494cdfc9645e9996a91d989124d92",
            "505d74a00ca94f54880f503c269f74fa",
            "98105746d3404fda893cc582a18fef03",
            "13efb60a13b341c2999e49d781d334c7",
            "9ced6a02c0d5472ebeaafd556979030d",
            "b22cebc3186c4970b65d87cfd08637b7",
            "d0abefe04f7641ecbca526dc7bacdd85",
            "53b12bf34e0f4f5189fd674017126c0c",
            "c604e096d4e14ea294e484d900016a82",
            "03a3d183e76047dfa517bd0ecb31f040",
            "871522f814ac4d9d88d03a6d688eb35f",
            "dc3900213e994d038d0a10610bee3b2d",
            "41f9a0871ac2402b83b6679c0ed719a0",
            "19d61f980174410b963264e3ea4e4032",
            "461eb6e9ec844568a5a47165059b09c3",
            "6c406b881dc648a5ad996e6d76ea9432",
            "5ed718c88e954c38bab883b783d8cc6c",
            "86717ab8ac544ab9aa238017a7edee2e",
            "84d94db589a349a7a3e96b37b26e81f2",
            "b12c67b221c14e75bed7a0c8e247478a"
          ]
        },
        "outputId": "6a7c6549-c986-4ca5-8dfa-0fa8812de8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 1.4G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 0.0 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:01<00:00, 22.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving Jim-202020/model/pytorch_model.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting qwen3 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q5_k_s'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at Jim-202020/model into f16 GGUF format.\n",
            "The output location will be /content/Jim-202020/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 6144\n",
            "INFO:hf-to-gguf:gguf: head count = 16\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151643\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/Jim-202020/model/unsloth.F16.gguf: n_tensors = 310, total_size = 3.4G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.44G/3.44G [01:38<00:00, 35.1Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/Jim-202020/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/Jim-202020/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_s. This might take 20 minutes...\n",
            "main: build = 5803 (d7f5f4e5)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/Jim-202020/model/unsloth.F16.gguf' to '/content/Jim-202020/model/unsloth.Q5_K_S.gguf' as Q5_K_S using 4 threads\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 310 tensors from /content/Jim-202020/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 1.7B\n",
            "llama_model_loader: - kv   4:                          qwen3.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                       qwen3.context_length u32              = 32768\n",
            "llama_model_loader: - kv   6:                     qwen3.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   7:                  qwen3.feed_forward_length u32              = 6144\n",
            "llama_model_loader: - kv   8:                 qwen3.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  113 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 310]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 310]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   3/ 310]                  blk.0.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   5/ 310]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 310]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   7/ 310]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   9/ 310]                  blk.0.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  10/ 310]                blk.0.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  11/ 310]                blk.0.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  12/ 310]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 310]                  blk.0.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  14/ 310]                  blk.1.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  16/ 310]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 310]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  18/ 310]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  20/ 310]                  blk.1.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  21/ 310]                blk.1.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  22/ 310]                blk.1.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  23/ 310]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 310]                  blk.1.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  25/ 310]                  blk.2.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  27/ 310]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 310]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  29/ 310]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  31/ 310]                  blk.2.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  32/ 310]                blk.2.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  33/ 310]                blk.2.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  34/ 310]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 310]                  blk.2.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  36/ 310]                  blk.3.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  38/ 310]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  39/ 310]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  40/ 310]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  42/ 310]                  blk.3.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  43/ 310]                blk.3.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  44/ 310]                blk.3.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  45/ 310]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  46/ 310]                  blk.3.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  47/ 310]                  blk.4.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  49/ 310]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 310]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  51/ 310]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  53/ 310]                  blk.4.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  54/ 310]                blk.4.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  55/ 310]                blk.4.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  56/ 310]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 310]                  blk.4.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  58/ 310]                  blk.5.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  60/ 310]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  61/ 310]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  62/ 310]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  64/ 310]                  blk.5.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  65/ 310]                blk.5.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  66/ 310]                blk.5.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  67/ 310]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 310]                  blk.5.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  69/ 310]                  blk.6.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  71/ 310]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 310]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  73/ 310]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  75/ 310]                  blk.6.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  76/ 310]                blk.6.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  77/ 310]                blk.6.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  78/ 310]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  79/ 310]                  blk.6.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  80/ 310]                  blk.7.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  82/ 310]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 310]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  84/ 310]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  86/ 310]                  blk.7.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  87/ 310]                blk.7.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  88/ 310]                blk.7.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  89/ 310]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 310]                  blk.7.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  91/ 310]                  blk.8.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  93/ 310]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 310]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  95/ 310]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  97/ 310]                  blk.8.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[  98/ 310]                blk.8.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[  99/ 310]                blk.8.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 100/ 310]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 101/ 310]                  blk.8.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 102/ 310]                  blk.9.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 104/ 310]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 106/ 310]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 108/ 310]                  blk.9.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 109/ 310]                blk.9.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 110/ 310]                blk.9.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 111/ 310]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 310]                  blk.9.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 113/ 310]                 blk.10.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 115/ 310]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 117/ 310]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 119/ 310]                 blk.10.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 120/ 310]               blk.10.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 121/ 310]               blk.10.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 122/ 310]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 123/ 310]                 blk.10.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 124/ 310]                 blk.11.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 126/ 310]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 128/ 310]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 130/ 310]                 blk.11.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 131/ 310]               blk.11.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 132/ 310]               blk.11.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 133/ 310]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 310]                 blk.11.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 135/ 310]                 blk.12.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 137/ 310]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 139/ 310]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 141/ 310]                 blk.12.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 142/ 310]               blk.12.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 143/ 310]               blk.12.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 144/ 310]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 145/ 310]                 blk.12.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 146/ 310]                 blk.13.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 148/ 310]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 150/ 310]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 152/ 310]                 blk.13.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 153/ 310]               blk.13.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 154/ 310]               blk.13.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 155/ 310]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 156/ 310]                 blk.13.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 157/ 310]                 blk.14.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 159/ 310]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 161/ 310]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 163/ 310]                 blk.14.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 164/ 310]               blk.14.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 165/ 310]               blk.14.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 166/ 310]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 167/ 310]                 blk.14.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 168/ 310]                 blk.15.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 170/ 310]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 172/ 310]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 174/ 310]                 blk.15.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 175/ 310]               blk.15.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 176/ 310]               blk.15.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 177/ 310]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 178/ 310]                 blk.15.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 179/ 310]                 blk.16.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 181/ 310]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 183/ 310]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 185/ 310]                 blk.16.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 186/ 310]               blk.16.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 187/ 310]               blk.16.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 188/ 310]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 189/ 310]                 blk.16.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 190/ 310]                 blk.17.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 192/ 310]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 194/ 310]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 196/ 310]                 blk.17.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 197/ 310]               blk.17.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 198/ 310]               blk.17.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 199/ 310]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 310]                 blk.17.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 201/ 310]                 blk.18.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 203/ 310]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 205/ 310]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 207/ 310]                 blk.18.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 208/ 310]               blk.18.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 209/ 310]               blk.18.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 210/ 310]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 211/ 310]                 blk.18.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 212/ 310]                 blk.19.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 214/ 310]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 216/ 310]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 218/ 310]                 blk.19.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 219/ 310]               blk.19.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 220/ 310]               blk.19.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 221/ 310]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 222/ 310]                 blk.19.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 223/ 310]                 blk.20.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 225/ 310]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 227/ 310]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 229/ 310]                 blk.20.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 230/ 310]               blk.20.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 231/ 310]               blk.20.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 232/ 310]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 233/ 310]                 blk.20.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 234/ 310]                 blk.21.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 236/ 310]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 238/ 310]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 240/ 310]                 blk.21.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 241/ 310]               blk.21.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 242/ 310]               blk.21.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 243/ 310]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 244/ 310]                 blk.21.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 245/ 310]                 blk.22.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 247/ 310]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 249/ 310]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 251/ 310]                 blk.22.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 252/ 310]               blk.22.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 253/ 310]               blk.22.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 254/ 310]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 255/ 310]                 blk.22.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 256/ 310]                 blk.23.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 258/ 310]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 260/ 310]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 262/ 310]                 blk.23.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 263/ 310]               blk.23.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 264/ 310]               blk.23.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 265/ 310]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 310]                 blk.23.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 267/ 310]                 blk.24.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 269/ 310]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 271/ 310]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 273/ 310]                 blk.24.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 274/ 310]               blk.24.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 275/ 310]               blk.24.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 276/ 310]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 277/ 310]                 blk.24.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 278/ 310]                 blk.25.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 280/ 310]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 282/ 310]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 284/ 310]                 blk.25.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 285/ 310]               blk.25.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 286/ 310]               blk.25.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 287/ 310]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 288/ 310]                 blk.25.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 289/ 310]                 blk.26.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 291/ 310]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 293/ 310]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 295/ 310]                 blk.26.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 296/ 310]               blk.26.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 297/ 310]               blk.26.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 298/ 310]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 299/ 310]                 blk.26.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 300/ 310]                 blk.27.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 302/ 310]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 304/ 310]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 306/ 310]                 blk.27.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q5_K .. size =     4.00 MiB ->     1.38 MiB\n",
            "[ 307/ 310]               blk.27.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 308/ 310]               blk.27.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "[ 309/ 310]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 310/ 310]                 blk.27.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB\n",
            "llama_model_quantize_impl: model size  =  3281.97 MB\n",
            "llama_model_quantize_impl: quant size  =  1167.90 MB\n",
            "\n",
            "main: quantize time = 177806.64 ms\n",
            "main:    total time = 177806.64 ms\n",
            "Unsloth: Conversion completed! Output location: /content/Jim-202020/model/unsloth.Q5_K_S.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24aad7ae8540467991f70365038b8a18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q5_K_S.gguf:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03a3d183e76047dfa517bd0ecb31f040"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/Jim-202020/model\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if True: model.push_to_hub_gguf(\"Jim-202020/model\", tokenizer, quantization_method = \"q5_k_s\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bad11d7a9d244afc9fffffcf01330ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_598fc44fe4854c09a77371815cc67e51",
              "IPY_MODEL_f83263b44668499197bcc772b99a7c45",
              "IPY_MODEL_9402af12abb34b71a9984bd3a86af750"
            ],
            "layout": "IPY_MODEL_190b1b82a8854fc8b6e193ec80757bbd"
          }
        },
        "598fc44fe4854c09a77371815cc67e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_720db6074a024734b561adb085cddc1b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cf91155369e48fda96a2fad458e785c",
            "value": ""
          }
        },
        "f83263b44668499197bcc772b99a7c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_627792f37d8d48e38a76ca94486d1fcf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a93b49d8ec874748bd8c4895bed76bf7",
            "value": 1
          }
        },
        "9402af12abb34b71a9984bd3a86af750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fcbb207100b49df9681128f723f273d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aea88ce443c74d37af4f8997cfd74cc9",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá‚Äá1.49it/s]\n"
          }
        },
        "190b1b82a8854fc8b6e193ec80757bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "720db6074a024734b561adb085cddc1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf91155369e48fda96a2fad458e785c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "627792f37d8d48e38a76ca94486d1fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93b49d8ec874748bd8c4895bed76bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fcbb207100b49df9681128f723f273d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea88ce443c74d37af4f8997cfd74cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e9253dfef14c289b908dc48e420cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_124f366608184718a366032b7add4744",
              "IPY_MODEL_d33cb514fdec4ae6829dcb110c0abf50",
              "IPY_MODEL_500123949e6e48ebbe2c0d248766ee77"
            ],
            "layout": "IPY_MODEL_0d47afcb4fc042a2a101930e5ef9e800"
          }
        },
        "124f366608184718a366032b7add4744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576da1f0a6dc4c83b2c048973a61e6cd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4849409d4374ebab6ab7b37b9764e20",
            "value": ""
          }
        },
        "d33cb514fdec4ae6829dcb110c0abf50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4269af5bf35c4175a2317174d6bbdd5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_449f78932a3a47c69427421d3e12d861",
            "value": 1
          }
        },
        "500123949e6e48ebbe2c0d248766ee77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ba287714994a0ea5b5cc69c78edf05",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_343032c628a94c8d90e86835ff25a197",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá‚Äá1.45it/s]\n"
          }
        },
        "0d47afcb4fc042a2a101930e5ef9e800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576da1f0a6dc4c83b2c048973a61e6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4849409d4374ebab6ab7b37b9764e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4269af5bf35c4175a2317174d6bbdd5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449f78932a3a47c69427421d3e12d861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9ba287714994a0ea5b5cc69c78edf05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "343032c628a94c8d90e86835ff25a197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97f66167a8bb49c69be54c1da70cef61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_079979b5c437498fa34dddc2defc3367",
              "IPY_MODEL_de4f6b11eef14245a1471b7d9e169eac",
              "IPY_MODEL_494f564cbfda4d5fb28fb241681964a8"
            ],
            "layout": "IPY_MODEL_6226d57df37e4cdc8007d8296daefee9"
          }
        },
        "079979b5c437498fa34dddc2defc3367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373ab21479024b85ab55dbf8749aaedc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_127ce1a9c2854d7086b8b709d3db0d97",
            "value": "Capturing‚ÄáCUDA‚Äágraph‚Äáshapes:‚Äá100%"
          }
        },
        "de4f6b11eef14245a1471b7d9e169eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb9a00750b5f42929acf202ca90f6323",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f1fb93f5b8f4102902d093a8b5177a3",
            "value": 27
          }
        },
        "494f564cbfda4d5fb28fb241681964a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d6902547261486697ea1c8a016999cd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_93b233580f2d44219c0bc6479775d775",
            "value": "‚Äá27/27‚Äá[00:43&lt;00:00,‚Äá‚Äá1.61s/it]"
          }
        },
        "6226d57df37e4cdc8007d8296daefee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373ab21479024b85ab55dbf8749aaedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "127ce1a9c2854d7086b8b709d3db0d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb9a00750b5f42929acf202ca90f6323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1fb93f5b8f4102902d093a8b5177a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d6902547261486697ea1c8a016999cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b233580f2d44219c0bc6479775d775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4dbf4b11b943cda8e0f2666a50dc79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df0fad891c4e471d9808039527152cdf",
              "IPY_MODEL_a95866b928dc48d8a23e354b4ce1a408",
              "IPY_MODEL_de9a6ebf23f345beba2624cb1d701201"
            ],
            "layout": "IPY_MODEL_bac92007aced4a1caa5882dba69544f1"
          }
        },
        "df0fad891c4e471d9808039527152cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5925b6505cd401688e5ff68cd73053b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4981ec6235904e35a25523033b19221a",
            "value": "README.md:‚Äá100%"
          }
        },
        "a95866b928dc48d8a23e354b4ce1a408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2996111558574bb8b91a7c10772cf403",
            "max": 603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1533645f5fe848c0bea21d0975339979",
            "value": 603
          }
        },
        "de9a6ebf23f345beba2624cb1d701201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad21692a23ce423aa3230fdaf3d3b386",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85361b7f84fa4b799b817e815f201341",
            "value": "‚Äá603/603‚Äá[00:00&lt;00:00,‚Äá16.0kB/s]"
          }
        },
        "bac92007aced4a1caa5882dba69544f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5925b6505cd401688e5ff68cd73053b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4981ec6235904e35a25523033b19221a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2996111558574bb8b91a7c10772cf403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1533645f5fe848c0bea21d0975339979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad21692a23ce423aa3230fdaf3d3b386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85361b7f84fa4b799b817e815f201341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be02c75a772d46228f02173078551d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_327d98f474854975800711d2e1325738",
              "IPY_MODEL_92a429a93084473a8dc51515ef06d9de",
              "IPY_MODEL_4156369fbf2440dea7b54f2a35282445"
            ],
            "layout": "IPY_MODEL_c33ac79a5814407a8361fbac5cd77d1e"
          }
        },
        "327d98f474854975800711d2e1325738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3beb0a60e6444bf38a6cb8b7cc05c8be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4ca494dc6f9549bb8a0e4b50e054d91c",
            "value": "data/cot-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "92a429a93084473a8dc51515ef06d9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2710b08e2a34211baedbc331166e790",
            "max": 105878062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_feae6f95c79245cf9d631ec9b879c197",
            "value": 105878062
          }
        },
        "4156369fbf2440dea7b54f2a35282445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df410c2c825a457ca0e66a6e2005d5e2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dfb1cb75ddfd4b18a5aea52c1a1a625c",
            "value": "‚Äá106M/106M‚Äá[00:01&lt;00:00,‚Äá78.2MB/s]"
          }
        },
        "c33ac79a5814407a8361fbac5cd77d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3beb0a60e6444bf38a6cb8b7cc05c8be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca494dc6f9549bb8a0e4b50e054d91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2710b08e2a34211baedbc331166e790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feae6f95c79245cf9d631ec9b879c197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df410c2c825a457ca0e66a6e2005d5e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb1cb75ddfd4b18a5aea52c1a1a625c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d3b1e12dbec449396dd098133eabfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_145cf3c6274743d3bb1fb4d5051d27b6",
              "IPY_MODEL_4b1cabf7834a413d91d163777d99c5c2",
              "IPY_MODEL_9114470a123b4846bef5bbb8f9e60115"
            ],
            "layout": "IPY_MODEL_cf67628ba2be427d96f567202b709e67"
          }
        },
        "145cf3c6274743d3bb1fb4d5051d27b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230d9141d0f6478d8ee3bcf4d4db8760",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_307ef9d17ea14654b863a3d89e3b8cfa",
            "value": "Generating‚Äácot‚Äásplit:‚Äá100%"
          }
        },
        "4b1cabf7834a413d91d163777d99c5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7dd6ec737f34848b8aa82a1fbf1e2ae",
            "max": 19252,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b3aa619deb4db98cc6709bd245b086",
            "value": 19252
          }
        },
        "9114470a123b4846bef5bbb8f9e60115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44fe098cbb7c4e03bf516ba9f79a656e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc45027eadea4fb1a0b2cde4a533e89e",
            "value": "‚Äá19252/19252‚Äá[00:06&lt;00:00,‚Äá2774.28‚Äáexamples/s]"
          }
        },
        "cf67628ba2be427d96f567202b709e67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "230d9141d0f6478d8ee3bcf4d4db8760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307ef9d17ea14654b863a3d89e3b8cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7dd6ec737f34848b8aa82a1fbf1e2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b3aa619deb4db98cc6709bd245b086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44fe098cbb7c4e03bf516ba9f79a656e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc45027eadea4fb1a0b2cde4a533e89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a501bcd9abe7477ebe38c981d89ac091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37d9daf823de43caa7be1916f862d7d9",
              "IPY_MODEL_f1c313adf0164cc49b3051489105e90e",
              "IPY_MODEL_b164b9fd9a5f49cc86814efcbe4386be"
            ],
            "layout": "IPY_MODEL_a4a40a8ef0fe45c29a287a8da5144c3d"
          }
        },
        "37d9daf823de43caa7be1916f862d7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e39504676e824c7da81f9e653f25824e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61b6d4244ab14c668a530e75919e79a0",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "f1c313adf0164cc49b3051489105e90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9cd78ad931f43cca514b31da6ee2813",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a649c1cc87d4f2ab4f6794850500343",
            "value": 64
          }
        },
        "b164b9fd9a5f49cc86814efcbe4386be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9435ca5b5574041a45819446e502f69",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5f951cac3fb64a22939096e2ed01cd53",
            "value": "‚Äá64/64‚Äá[00:01&lt;00:00,‚Äá44.15‚Äáexamples/s]"
          }
        },
        "a4a40a8ef0fe45c29a287a8da5144c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e39504676e824c7da81f9e653f25824e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61b6d4244ab14c668a530e75919e79a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9cd78ad931f43cca514b31da6ee2813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a649c1cc87d4f2ab4f6794850500343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9435ca5b5574041a45819446e502f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f951cac3fb64a22939096e2ed01cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e13868e2b8b342acb1d1dac9bad36b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8721fc5178714799a1e16cf7bfe36d31",
              "IPY_MODEL_e2356755ed6b4898a315f5b62e49d5bf",
              "IPY_MODEL_d0c3e0c193514f10b8f63221a99d221c"
            ],
            "layout": "IPY_MODEL_ed83b26c9be744ef83bb8569e5475cd7"
          }
        },
        "8721fc5178714799a1e16cf7bfe36d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614f33069c59485c99d6dc5a5370f56f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6e838e62789d46979cab90e85c4a7f74",
            "value": "README.md:‚Äá"
          }
        },
        "e2356755ed6b4898a315f5b62e49d5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd19f6e1fc0747049a3d0ad2659a81ad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61fcd3b4b7ba4a16a3e197fa1e346caa",
            "value": 1
          }
        },
        "d0c3e0c193514f10b8f63221a99d221c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d094aae6a345e79e145bcd45108164",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a2ae13fef001462e89bc780dc40a0a39",
            "value": "‚Äá3.44k/?‚Äá[00:00&lt;00:00,‚Äá235kB/s]"
          }
        },
        "ed83b26c9be744ef83bb8569e5475cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "614f33069c59485c99d6dc5a5370f56f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e838e62789d46979cab90e85c4a7f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd19f6e1fc0747049a3d0ad2659a81ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "61fcd3b4b7ba4a16a3e197fa1e346caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33d094aae6a345e79e145bcd45108164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ae13fef001462e89bc780dc40a0a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7993e1dae6a4509b45edb399fe0fe6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af77c83cdbb3462e83e5846d2c7d7907",
              "IPY_MODEL_a3410b6d3b114148b9808b6db1cd4b17",
              "IPY_MODEL_7269ab3382af4ac699a3ffa933b152d2"
            ],
            "layout": "IPY_MODEL_6fca825c104c4973a9f397e5fa0c21f7"
          }
        },
        "af77c83cdbb3462e83e5846d2c7d7907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bba12a7613b41749b162c624332e9f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d682db7c3016412ba857305efeab5b52",
            "value": "en/train-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "a3410b6d3b114148b9808b6db1cd4b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c8ec58e71344c0b1fd42f311a8f763",
            "max": 5232060,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e09732c0d76c42928395f37a62342e32",
            "value": 5232060
          }
        },
        "7269ab3382af4ac699a3ffa933b152d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cb4e7c57481488ab6906325e49a7e85",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1506c94c991c49b89f4e1081c1f4177a",
            "value": "‚Äá5.23M/5.23M‚Äá[00:01&lt;00:00,‚Äá3.85MB/s]"
          }
        },
        "6fca825c104c4973a9f397e5fa0c21f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bba12a7613b41749b162c624332e9f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d682db7c3016412ba857305efeab5b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5c8ec58e71344c0b1fd42f311a8f763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e09732c0d76c42928395f37a62342e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cb4e7c57481488ab6906325e49a7e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1506c94c991c49b89f4e1081c1f4177a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7b7b85fa3846c1a72fc5be043670aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d13dab1fcea84cc88c16a2617a7856c2",
              "IPY_MODEL_d16f0dfef7b24a5cba715442de407d50",
              "IPY_MODEL_e038d9ddb660496797a7579faad7931b"
            ],
            "layout": "IPY_MODEL_ff092d18246748f5a4497bd57f4cd889"
          }
        },
        "d13dab1fcea84cc88c16a2617a7856c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9c994403eb84556aaad81785dc487ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_831e710726cb43eeb7e31852f29c2f73",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "d16f0dfef7b24a5cba715442de407d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5646273504b84604a3f5ded1a18c4aff",
            "max": 14116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e478224a08a14d45bba1ed9ba075aece",
            "value": 14116
          }
        },
        "e038d9ddb660496797a7579faad7931b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67fdca41df4d4e97bcf40e803d7bc03d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_913133021baf4f9eaae2216334699e35",
            "value": "‚Äá14116/14116‚Äá[00:00&lt;00:00,‚Äá168920.47‚Äáexamples/s]"
          }
        },
        "ff092d18246748f5a4497bd57f4cd889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9c994403eb84556aaad81785dc487ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831e710726cb43eeb7e31852f29c2f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5646273504b84604a3f5ded1a18c4aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e478224a08a14d45bba1ed9ba075aece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67fdca41df4d4e97bcf40e803d7bc03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913133021baf4f9eaae2216334699e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8639778fc0004279a76cffdd77619765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_215cfcb2a3eb4fc9b7902c4d9afe91e0",
              "IPY_MODEL_c5c899c824264437a11cf0ed81e59958",
              "IPY_MODEL_766fbb2b1bbf4fd1b2d9cc01d3ceacb4"
            ],
            "layout": "IPY_MODEL_bede7de886714d8883977e8622cd47fc"
          }
        },
        "215cfcb2a3eb4fc9b7902c4d9afe91e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bc15faed9a4d9299393dbcd52e3c58",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_26c2ed00a1e447e88a8fc4dac63c2775",
            "value": "Map:‚Äá100%"
          }
        },
        "c5c899c824264437a11cf0ed81e59958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11979ef4af6a470d908344c7020e442f",
            "max": 14116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00ee4e52f4b748c0835904117f674014",
            "value": 14116
          }
        },
        "766fbb2b1bbf4fd1b2d9cc01d3ceacb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1307018d7041f78b936dd644e36526",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c472138a01c45d0bb07b802a3bf1456",
            "value": "‚Äá14116/14116‚Äá[00:02&lt;00:00,‚Äá2695.82‚Äáexamples/s]"
          }
        },
        "bede7de886714d8883977e8622cd47fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91bc15faed9a4d9299393dbcd52e3c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c2ed00a1e447e88a8fc4dac63c2775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11979ef4af6a470d908344c7020e442f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ee4e52f4b748c0835904117f674014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f1307018d7041f78b936dd644e36526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c472138a01c45d0bb07b802a3bf1456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4044aa95a2414c02bb83f8ac17ebd03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12d92645a9f44063b17e06ee4573337b",
              "IPY_MODEL_7c8d5fdf5212465691b792babd8489e2",
              "IPY_MODEL_f251d57364cd42dfbf62fd4e0294a2c8"
            ],
            "layout": "IPY_MODEL_4d19bc46991a406881a433fd6af3ec9e"
          }
        },
        "12d92645a9f44063b17e06ee4573337b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15999d7b09ea48cabe8eaff40d78f6c2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1c16f16ac5b4425aa347e49d2439b59e",
            "value": "Map:‚Äá100%"
          }
        },
        "7c8d5fdf5212465691b792babd8489e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb1e32521d0442d8e5f976a652204d5",
            "max": 14116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8d31b2a43924a6d9af3a31badea7aa8",
            "value": 14116
          }
        },
        "f251d57364cd42dfbf62fd4e0294a2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c997e8f0ffc48aeb431d1d06fd50cff",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9b6c9d8e84b44390b5791c0cb9ffcfe4",
            "value": "‚Äá14116/14116‚Äá[00:14&lt;00:00,‚Äá1275.72‚Äáexamples/s]"
          }
        },
        "4d19bc46991a406881a433fd6af3ec9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15999d7b09ea48cabe8eaff40d78f6c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c16f16ac5b4425aa347e49d2439b59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfb1e32521d0442d8e5f976a652204d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d31b2a43924a6d9af3a31badea7aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c997e8f0ffc48aeb431d1d06fd50cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6c9d8e84b44390b5791c0cb9ffcfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b128f6bd9b441dfb9ecac2bda23548c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a27419f54130440f9f306cb57830a9c0",
              "IPY_MODEL_e01fc14670784aebac6aa1a50f30f432",
              "IPY_MODEL_e824618c951b4bb08a019ae77bb97956"
            ],
            "layout": "IPY_MODEL_d132a412b7cb400095943f7587c0178d"
          }
        },
        "a27419f54130440f9f306cb57830a9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9e31241999f4a6f8756a0c02b221172",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_81f2815b04b146d79b8693338f936fc2",
            "value": "Map:‚Äá100%"
          }
        },
        "e01fc14670784aebac6aa1a50f30f432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3055147c72214b70bf440bc0002bf0c7",
            "max": 14116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9215322de5de4009a3d4ae104bd795ef",
            "value": 14116
          }
        },
        "e824618c951b4bb08a019ae77bb97956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85406bbe48df49629b4f2901203bf534",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c0009b32a6c74890aaa707349e5e07c6",
            "value": "‚Äá14116/14116‚Äá[00:07&lt;00:00,‚Äá2040.90‚Äáexamples/s]"
          }
        },
        "d132a412b7cb400095943f7587c0178d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9e31241999f4a6f8756a0c02b221172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f2815b04b146d79b8693338f936fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3055147c72214b70bf440bc0002bf0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9215322de5de4009a3d4ae104bd795ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85406bbe48df49629b4f2901203bf534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0009b32a6c74890aaa707349e5e07c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d03bb3856be487b99c2903a91cd5177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9fe0150dc2946cf994a6a21547acbd9",
              "IPY_MODEL_dbe05ad284444e029cfc863e85c41b6e",
              "IPY_MODEL_5165bf9b70604c39b31d5b10ef5485ef"
            ],
            "layout": "IPY_MODEL_491f3e2b9ddb4a209398c455a26f9690"
          }
        },
        "d9fe0150dc2946cf994a6a21547acbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e514bdc68d1b490a931b306f4facc0ac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_44f3b417a33740258f8441e94dc56405",
            "value": "Processed‚Äáprompts:‚Äá100%"
          }
        },
        "dbe05ad284444e029cfc863e85c41b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973516dd21944bc596b55e4f4cef541b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c89c2e0c5da458991c3252e619f253d",
            "value": 1
          }
        },
        "5165bf9b70604c39b31d5b10ef5485ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c3a972318e465291256f7a6e887634",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e5b0c75e2d745e98f127b4456d18ed1",
            "value": "‚Äá1/1‚Äá[00:18&lt;00:00,‚Äá18.43s/it,‚Äáest.‚Äáspeed‚Äáinput:‚Äá0.54‚Äátoks/s,‚Äáoutput:‚Äá55.56‚Äátoks/s]"
          }
        },
        "491f3e2b9ddb4a209398c455a26f9690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "e514bdc68d1b490a931b306f4facc0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f3b417a33740258f8441e94dc56405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "973516dd21944bc596b55e4f4cef541b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c89c2e0c5da458991c3252e619f253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97c3a972318e465291256f7a6e887634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5b0c75e2d745e98f127b4456d18ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cf71211397d417488cf05d219027732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_374ef60446054831933b1c02e79ea51d",
              "IPY_MODEL_a1a2658f45bf43fd890429d1f124bcf3",
              "IPY_MODEL_f6eb3c95316c4e6180938ab6a208449f"
            ],
            "layout": "IPY_MODEL_4d589b124fc24aa5a2753c27bf9e31d5"
          }
        },
        "374ef60446054831933b1c02e79ea51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375635be6fe34dbfb990eafaa7e3b49c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a7b6bd2dfecc44039ab7b14fd2f5ffe5",
            "value": "Processed‚Äáprompts:‚Äá100%"
          }
        },
        "a1a2658f45bf43fd890429d1f124bcf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b97a564f80b44de8ab65f5f5f271c143",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f637dc9d1c3a464cba9acfd1aac9369e",
            "value": 1
          }
        },
        "f6eb3c95316c4e6180938ab6a208449f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53b353cc48ab498c8e9f5f5d9ddbce01",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_53df2796a5374d7eaf2149574c6668e4",
            "value": "‚Äá1/1‚Äá[00:41&lt;00:00,‚Äá41.48s/it,‚Äáest.‚Äáspeed‚Äáinput:‚Äá1.16‚Äátoks/s,‚Äáoutput:‚Äá48.24‚Äátoks/s]"
          }
        },
        "4d589b124fc24aa5a2753c27bf9e31d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "375635be6fe34dbfb990eafaa7e3b49c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b6bd2dfecc44039ab7b14fd2f5ffe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b97a564f80b44de8ab65f5f5f271c143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f637dc9d1c3a464cba9acfd1aac9369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53b353cc48ab498c8e9f5f5d9ddbce01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53df2796a5374d7eaf2149574c6668e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24aad7ae8540467991f70365038b8a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85f21060fda64ee2951a30966aa17b3c",
              "IPY_MODEL_61d494cdfc9645e9996a91d989124d92",
              "IPY_MODEL_505d74a00ca94f54880f503c269f74fa"
            ],
            "layout": "IPY_MODEL_98105746d3404fda893cc582a18fef03"
          }
        },
        "85f21060fda64ee2951a30966aa17b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13efb60a13b341c2999e49d781d334c7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ced6a02c0d5472ebeaafd556979030d",
            "value": "100%"
          }
        },
        "61d494cdfc9645e9996a91d989124d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b22cebc3186c4970b65d87cfd08637b7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0abefe04f7641ecbca526dc7bacdd85",
            "value": 1
          }
        },
        "505d74a00ca94f54880f503c269f74fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53b12bf34e0f4f5189fd674017126c0c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c604e096d4e14ea294e484d900016a82",
            "value": "‚Äá1/1‚Äá[00:11&lt;00:00,‚Äá11.73s/it]"
          }
        },
        "98105746d3404fda893cc582a18fef03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13efb60a13b341c2999e49d781d334c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ced6a02c0d5472ebeaafd556979030d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b22cebc3186c4970b65d87cfd08637b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0abefe04f7641ecbca526dc7bacdd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53b12bf34e0f4f5189fd674017126c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c604e096d4e14ea294e484d900016a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03a3d183e76047dfa517bd0ecb31f040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_871522f814ac4d9d88d03a6d688eb35f",
              "IPY_MODEL_dc3900213e994d038d0a10610bee3b2d",
              "IPY_MODEL_41f9a0871ac2402b83b6679c0ed719a0"
            ],
            "layout": "IPY_MODEL_19d61f980174410b963264e3ea4e4032"
          }
        },
        "871522f814ac4d9d88d03a6d688eb35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461eb6e9ec844568a5a47165059b09c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c406b881dc648a5ad996e6d76ea9432",
            "value": "unsloth.Q5_K_S.gguf:‚Äá"
          }
        },
        "dc3900213e994d038d0a10610bee3b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed718c88e954c38bab883b783d8cc6c",
            "max": 1230579168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86717ab8ac544ab9aa238017a7edee2e",
            "value": 1230579168
          }
        },
        "41f9a0871ac2402b83b6679c0ed719a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d94db589a349a7a3e96b37b26e81f2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b12c67b221c14e75bed7a0c8e247478a",
            "value": "‚Äá1.23G/?‚Äá[00:11&lt;00:00,‚Äá426MB/s]"
          }
        },
        "19d61f980174410b963264e3ea4e4032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461eb6e9ec844568a5a47165059b09c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c406b881dc648a5ad996e6d76ea9432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed718c88e954c38bab883b783d8cc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86717ab8ac544ab9aa238017a7edee2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84d94db589a349a7a3e96b37b26e81f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b12c67b221c14e75bed7a0c8e247478a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}